# üìñ Gu√≠a Completa de Par√°metros de Machine Learning

## Tabla de Contenidos
1. [Par√°metros de Modelos](#par√°metros-de-modelos)
2. [Par√°metros de Evaluaci√≥n](#par√°metros-de-evaluaci√≥n)
3. [Par√°metros de Validaci√≥n Cruzada](#par√°metros-de-validaci√≥n-cruzada)
4. [Par√°metros de GridSearchCV](#par√°metros-de-gridsearchcv)
5. [Par√°metros de Visualizaci√≥n](#par√°metros-de-visualizaci√≥n)

---

## Par√°metros de Modelos

### üå≥ Random Forest

```python
RandomForestClassifier(
    n_estimators=100,        # ¬øQu√© significa?
    max_depth=None,          # ¬øPara qu√© sirve?
    min_samples_split=2,     # ¬øC√≥mo afecta?
    min_samples_leaf=1,      # ¬øCu√°ndo cambiar?
    random_state=42          # ¬øPor qu√© 42?
)
```

#### **n_estimators** (N√∫mero de √°rboles)
**¬øQu√© es?** El n√∫mero de √°rboles en el bosque.

```python
# ANALOG√çA: Imagina que quieres decidir qu√© pel√≠cula ver
# Una persona te da una opini√≥n ‚Üí Podr√≠a estar equivocada
# 100 personas te dan opiniones ‚Üí La mayor√≠a probablemente acierta

# Ejemplo pr√°ctico
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
import numpy as np

X, y = make_classification(n_samples=1000, n_features=4, n_classes=2, random_state=42)

# Probar diferentes n√∫meros de √°rboles
n_trees = [1, 5, 10, 25, 50, 100, 200]
scores = []

for n in n_trees:
    rf = RandomForestClassifier(n_estimators=n, random_state=42)
    rf.fit(X, y)
    score = rf.score(X, y)
    scores.append(score)
    print(f"üå≥ Con {n:3d} √°rboles: Accuracy = {score:.3f}")

# Visualizar
plt.figure(figsize=(10, 6))
plt.plot(n_trees, scores, 'o-', linewidth=2, markersize=8)
plt.xlabel('N√∫mero de √Årboles (n_estimators)')
plt.ylabel('Accuracy')
plt.title('¬øM√°s √°rboles = Mejor rendimiento?')
plt.grid(True, alpha=0.3)
plt.show()

print("\nüí° REGLAS PR√ÅCTICAS:")
print("‚Ä¢ 10-50 √°rboles: Para experimentar r√°pido")
print("‚Ä¢ 100 √°rboles: Buen punto de partida")
print("‚Ä¢ 500-1000 √°rboles: Para m√°ximo rendimiento (m√°s lento)")
print("‚Ä¢ M√°s √°rboles = mejor rendimiento, pero m√°s tiempo")
```

#### **max_depth** (Profundidad m√°xima)
**¬øQu√© es?** Qu√© tan profundo puede crecer cada √°rbol.

```python
# ANALOG√çA: Es como un cuestionario
# max_depth=1: Solo 1 pregunta ‚Üí "¬øEres alto?" ‚Üí S√≠/No
# max_depth=3: Hasta 3 preguntas ‚Üí "¬øEres alto?" ‚Üí "¬øPracticas deporte?" ‚Üí "¬øComes verduras?"

# Ejemplo visual
from sklearn.tree import DecisionTreeClassifier, plot_tree

# Datos simples para visualizar
X_simple = [[160, 50], [180, 70], [165, 55], [175, 80], [155, 45]]  # [altura, peso]
y_simple = [0, 1, 0, 1, 0]  # 0=No deportista, 1=Deportista

fig, axes = plt.subplots(1, 3, figsize=(18, 6))

depths = [1, 3, None]
titles = ["Muy Simple (max_depth=1)", "Equilibrado (max_depth=3)", "Sin l√≠mite (max_depth=None)"]

for i, (depth, title) in enumerate(zip(depths, titles)):
    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)
    tree.fit(X_simple, y_simple)

    plot_tree(tree, ax=axes[i], feature_names=['Altura', 'Peso'],
              class_names=['No Deportista', 'Deportista'], filled=True)
    axes[i].set_title(title)

plt.tight_layout()
plt.show()

print("\nüéØ EFECTOS DE max_depth:")
print("‚Ä¢ max_depth=1-3: Modelo simple, puede ser underfitting")
print("‚Ä¢ max_depth=5-10: Equilibrado para la mayor√≠a de problemas")
print("‚Ä¢ max_depth=None: Sin l√≠mite, riesgo de overfitting")
print("‚Ä¢ Si tienes overfitting ‚Üí Reduce max_depth")
print("‚Ä¢ Si tienes underfitting ‚Üí Aumenta max_depth")
```

#### **min_samples_split** (M√≠nimas muestras para dividir)
**¬øQu√© es?** Cu√°ntos ejemplos debe tener un nodo para poder dividirse.

```python
# ANALOG√çA: Reglas para tomar decisiones en grupo
# min_samples_split=2: "Si somos 2 o m√°s, podemos dividir el grupo"
# min_samples_split=20: "Solo si somos 20 o m√°s, vale la pena dividir el grupo"

# Ejemplo pr√°ctico
def explain_min_samples_split():
    print("üå≤ SIMULACI√ìN DE DIVISI√ìN DE NODOS:")
    print("\nTenemos un nodo con diferentes cantidades de muestras:")

    scenarios = [
        (50, 2, "‚úÖ Se puede dividir (50 >= 2)"),
        (50, 20, "‚úÖ Se puede dividir (50 >= 20)"),
        (10, 20, "‚ùå NO se puede dividir (10 < 20)"),
        (5, 2, "‚úÖ Se puede dividir (5 >= 2)"),
    ]

    for samples, min_split, result in scenarios:
        print(f"  Muestras en el nodo: {samples:2d} | min_samples_split: {min_split:2d} ‚Üí {result}")

    print("\nüí° EFECTOS PR√ÅCTICOS:")
    print("‚Ä¢ min_samples_split BAJO (2-5): √Årboles m√°s profundos, m√°s overfitting")
    print("‚Ä¢ min_samples_split ALTO (20-50): √Årboles m√°s simples, menos overfitting")
    print("‚Ä¢ Para datasets peque√±os (<1000): usar valores bajos (2-5)")
    print("‚Ä¢ Para datasets grandes (>10000): usar valores altos (20-100)")

explain_min_samples_split()

# Ejemplo con c√≥digo
X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)

splits = [2, 10, 50, 100]
print(f"\nüß™ EXPERIMENTO - Efecto de min_samples_split:")

for split in splits:
    rf = RandomForestClassifier(n_estimators=100, min_samples_split=split, random_state=42)
    rf.fit(X, y)

    # Calcular profundidad promedio de los √°rboles
    depths = [tree.get_depth() for tree in rf.estimators_]
    avg_depth = np.mean(depths)

    score = rf.score(X, y)
    print(f"min_samples_split={split:3d} ‚Üí Profundidad promedio: {avg_depth:.1f} | Accuracy: {score:.3f}")
```

#### **min_samples_leaf** (M√≠nimas muestras en hoja)
**¬øQu√© es?** Cu√°ntos ejemplos como m√≠nimo debe tener una hoja (nodo final).

```python
# ANALOG√çA: Reglas para grupos finales
# Es como decir "Un grupo final debe tener al menos X personas para ser v√°lido"

def explain_min_samples_leaf():
    print("üçÉ SIMULACI√ìN DE HOJAS (NODOS FINALES):")
    print("\nEjemplos de lo que pasa con diferentes valores:")

    print("\nüîπ min_samples_leaf = 1:")
    print("  Permite hojas con solo 1 ejemplo")
    print("  Ejemplo: Hoja con 1 persona alta ‚Üí 'Todas las personas altas son deportistas'")
    print("  Riesgo: ¬°Puede ser casualidad!")

    print("\nüîπ min_samples_leaf = 10:")
    print("  Requiere al menos 10 ejemplos por hoja")
    print("  Ejemplo: Hoja con 10 personas altas ‚Üí 'Patr√≥n m√°s confiable'")
    print("  Beneficio: Predicciones m√°s generales")

    print("\nüîπ min_samples_leaf = 50:")
    print("  Requiere al menos 50 ejemplos por hoja")
    print("  Beneficio: Muy generalizable")
    print("  Riesgo: Podr√≠a ser demasiado simple")

explain_min_samples_leaf()

# Experimento pr√°ctico
X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)

leaf_sizes = [1, 5, 20, 50]
print(f"\nüß™ EXPERIMENTO - Efecto de min_samples_leaf:")

for leaf in leaf_sizes:
    rf = RandomForestClassifier(n_estimators=50, min_samples_leaf=leaf, random_state=42)
    rf.fit(X, y)

    # Contar hojas totales en todos los √°rboles
    total_leaves = sum([tree.get_n_leaves() for tree in rf.estimators_])
    avg_leaves = total_leaves / len(rf.estimators_)

    score = rf.score(X, y)
    print(f"min_samples_leaf={leaf:2d} ‚Üí Hojas promedio por √°rbol: {avg_leaves:5.1f} | Accuracy: {score:.3f}")

print(f"\nüí° OBSERVACIONES:")
print("‚Ä¢ M√°s hojas = Modelo m√°s complejo = Posible overfitting")
print("‚Ä¢ Menos hojas = Modelo m√°s simple = Posible underfitting")
```

#### **random_state** (Semilla aleatoria)
**¬øQu√© es?** Un n√∫mero que controla la aleatoriedad para obtener resultados reproducibles.

```python
# ANALOG√çA: Es como fijar la suerte en un videojuego
# Sin random_state: Cada vez que juegas, la suerte es diferente
# Con random_state=42: Cada vez que juegas, la suerte es exactamente igual

print("üé≤ DEMOSTRACION DE random_state:")
print("\nüîÑ SIN random_state (resultados aleatorios):")

# Sin fijar semilla - resultados diferentes cada vez
for i in range(3):
    rf = RandomForestClassifier(n_estimators=10)  # Sin random_state
    rf.fit(X, y)
    score = rf.score(X, y)
    print(f"Ejecuci√≥n {i+1}: Accuracy = {score:.4f}")

print("\nüîí CON random_state=42 (resultados reproducibles):")

# Con semilla fija - resultados id√©nticos cada vez
for i in range(3):
    rf = RandomForestClassifier(n_estimators=10, random_state=42)
    rf.fit(X, y)
    score = rf.score(X, y)
    print(f"Ejecuci√≥n {i+1}: Accuracy = {score:.4f}")

print(f"\n‚ùì ¬øPOR QU√â USAR random_state?")
print("‚Ä¢ Para poder comparar modelos de forma justa")
print("‚Ä¢ Para que otros puedan reproducir tus resultados")
print("‚Ä¢ Para debugging (encontrar errores)")
print("‚Ä¢ El n√∫mero (42, 0, 123) no importa, solo que sea siempre el mismo")

print(f"\nüéØ CU√ÅNDO USAR:")
print("‚Ä¢ ‚úÖ Siempre en experimentos y comparaciones")
print("‚Ä¢ ‚úÖ En c√≥digo de producci√≥n para consistencia")
print("‚Ä¢ ‚ùå Puedes quitarlo en el modelo final si quieres m√°xima aleatoriedad")
```

### üîç Logistic Regression

```python
LogisticRegression(
    C=1.0,                   # Fuerza de regularizaci√≥n
    max_iter=100,            # M√°ximo n√∫mero de iteraciones
    solver='lbfgs',          # Algoritmo de optimizaci√≥n
    penalty='l2'             # Tipo de regularizaci√≥n
)
```

#### **C** (Par√°metro de regularizaci√≥n inverso)
**¬øQu√© es?** Controla qu√© tan estricto es el modelo (inverso de regularizaci√≥n).

```python
# ANALOG√çA: Imagina que eres un profesor calificando ex√°menes
# C alto (C=10): Profesor muy permisivo, acepta respuestas complejas
# C bajo (C=0.1): Profesor muy estricto, solo acepta respuestas simples

from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
import numpy as np
import matplotlib.pyplot as plt

# Crear datos con ruido para ver el efecto
X, y = make_classification(n_samples=100, n_features=2, n_redundant=0,
                         n_informative=2, n_clusters_per_class=1, random_state=42)

# Probar diferentes valores de C
C_values = [0.01, 0.1, 1.0, 10.0, 100.0]

fig, axes = plt.subplots(1, 5, figsize=(20, 4))

for i, C in enumerate(C_values):
    lr = LogisticRegression(C=C, random_state=42)
    lr.fit(X, y)

    # Crear malla para visualizar la frontera de decisi√≥n
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    Z = lr.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    axes[i].contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdBu)
    axes[i].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu)
    axes[i].set_title(f'C = {C}')

    # Mostrar los coeficientes (complejidad del modelo)
    coef_magnitude = np.linalg.norm(lr.coef_)
    axes[i].set_xlabel(f'|Coeficientes| = {coef_magnitude:.2f}')

plt.tight_layout()
plt.show()

print("üéØ INTERPRETACI√ìN:")
print("‚Ä¢ C BAJO (0.01-0.1): Modelo simple, frontera suave, menos overfitting")
print("‚Ä¢ C MEDIO (1.0): Balance equilibrado (valor por defecto)")
print("‚Ä¢ C ALTO (10-100): Modelo complejo, frontera irregular, riesgo overfitting")

# Ejemplo num√©rico
print(f"\nüìä EXPERIMENTO NUM√âRICO:")
for C in [0.1, 1.0, 10.0]:
    lr = LogisticRegression(C=C, random_state=42)
    lr.fit(X, y)

    train_score = lr.score(X, y)
    coef_size = np.linalg.norm(lr.coef_)

    print(f"C={C:4.1f} ‚Üí Accuracy: {train_score:.3f} | Tama√±o coeficientes: {coef_size:.2f}")
```

#### **max_iter** (M√°ximo de iteraciones)
**¬øQu√© es?** Cu√°ntas veces el algoritmo intenta mejorar la soluci√≥n.

```python
# ANALOG√çA: Es como estudiar para un examen
# max_iter=10: Solo estudias 10 sesiones ‚Üí Podr√≠as no estar listo
# max_iter=1000: Estudias 1000 sesiones ‚Üí Definitivamente estar√°s listo

print("üîÑ SIMULACION DE CONVERGENCIA:")

# Crear un problema m√°s dif√≠cil para ver la convergencia
X_hard, y_hard = make_classification(n_samples=1000, n_features=20,
                                   n_informative=10, n_redundant=5, random_state=42)

iterations = [10, 50, 100, 500, 1000]

print("Probando diferentes l√≠mites de iteraciones:")
for max_it in iterations:
    try:
        lr = LogisticRegression(max_iter=max_it, random_state=42)
        lr.fit(X_hard, y_hard)

        # Verificar si convergi√≥
        n_iter = lr.n_iter_[0] if hasattr(lr, 'n_iter_') else "N/A"
        score = lr.score(X_hard, y_hard)

        status = "‚úÖ Convergi√≥" if n_iter < max_it else "‚ö†Ô∏è No convergi√≥"
        print(f"max_iter={max_it:4d} ‚Üí Iteraciones usadas: {n_iter:3} | {status} | Accuracy: {score:.4f}")

    except Exception as e:
        print(f"max_iter={max_it:4d} ‚Üí ‚ùå Error: {str(e)[:50]}...")

print(f"\nüí° RECOMENDACIONES:")
print("‚Ä¢ Para datasets peque√±os: max_iter=100 suele ser suficiente")
print("‚Ä¢ Para datasets grandes: max_iter=1000 o m√°s")
print("‚Ä¢ Si ves warnings de convergencia: aumenta max_iter")
print("‚Ä¢ M√°s iteraciones = m√°s tiempo de entrenamiento")
```

#### **solver** (Algoritmo de optimizaci√≥n)
**¬øQu√© es?** El m√©todo que usa para encontrar la mejor soluci√≥n.

```python
# ANALOG√çA: Diferentes formas de llegar a un destino
# 'lbfgs': Como GPS inteligente, encuentra la ruta r√°pida (datasets peque√±os)
# 'saga': Como explorador paciente, funciona con cualquier terreno (datasets grandes)
# 'liblinear': Como taxi experimentado, bueno para rutas conocidas (problemas lineales)

from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
import time

# Crear datasets de diferentes tama√±os
datasets = {
    'Peque√±o (100 muestras)': make_classification(n_samples=100, n_features=10, random_state=42),
    'Mediano (1000 muestras)': make_classification(n_samples=1000, n_features=10, random_state=42),
    'Grande (5000 muestras)': make_classification(n_samples=5000, n_features=10, random_state=42)
}

solvers = ['lbfgs', 'liblinear', 'saga']

print("üèÉ‚Äç‚ôÇÔ∏è COMPARACI√ìN DE SOLVERS:")
print("=" * 60)

for dataset_name, (X, y) in datasets.items():
    print(f"\nüìä {dataset_name}:")

    for solver in solvers:
        try:
            start_time = time.time()

            lr = LogisticRegression(solver=solver, max_iter=1000, random_state=42)

            # Solo usar validaci√≥n cruzada para datasets no muy grandes
            if X.shape[0] <= 1000:
                scores = cross_val_score(lr, X, y, cv=3)
                avg_score = scores.mean()
            else:
                lr.fit(X, y)
                avg_score = lr.score(X, y)

            time_taken = time.time() - start_time

            print(f"  {solver:12} ‚Üí Accuracy: {avg_score:.4f} | Tiempo: {time_taken:.3f}s")

        except Exception as e:
            print(f"  {solver:12} ‚Üí ‚ùå Error: {str(e)[:30]}...")

print(f"\nüéØ GU√çA DE SELECCI√ìN:")
print("‚Ä¢ 'lbfgs': DEFAULT - Bueno para datasets peque√±os-medianos (<10k muestras)")
print("‚Ä¢ 'liblinear': R√°pido para problemas binarios y datasets peque√±os")
print("‚Ä¢ 'saga': Mejor para datasets grandes (>10k muestras)")
print("‚Ä¢ 'newton-cg': Alternativa a lbfgs para algunos casos")
print("‚Ä¢ 'sag': Similar a saga pero solo para datos densos")
```

### ü§ñ SVM (Support Vector Machine)

```python
SVC(
    C=1.0,                   # Par√°metro de regularizaci√≥n
    kernel='rbf',            # Tipo de kernel
    gamma='scale',           # Par√°metro del kernel
    probability=False        # Calcular probabilidades
)
```

#### **C** (Par√°metro de regularizaci√≥n)
**¬øQu√© es?** Controla el balance entre margen amplio y clasificar correctamente.

```python
# ANALOG√çA: Imagina que est√°s separando dos grupos de personas con una cuerda
# C BAJO: "No me importa si algunas personas quedan del lado equivocado,
#         lo importante es que la cuerda est√© bien centrada"
# C ALTO: "Quiero clasificar PERFECTAMENTE a cada persona,
#         aunque la cuerda tenga que hacer curvas raras"

from sklearn.svm import SVC
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
import numpy as np

# Crear datos con algo de ruido/overlap
X, y = make_classification(n_samples=100, n_features=2, n_redundant=0,
                         n_informative=2, n_clusters_per_class=1,
                         class_sep=0.8, random_state=42)

C_values = [0.1, 1.0, 10.0, 100.0]

fig, axes = plt.subplots(1, 4, figsize=(20, 5))

for i, C in enumerate(C_values):
    svm = SVC(C=C, kernel='rbf', random_state=42)
    svm.fit(X, y)

    # Visualizar frontera de decisi√≥n
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    axes[i].contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdBu)
    axes[i].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu, edgecolors='black')

    # Marcar vectores de soporte
    support_vectors = svm.support_vectors_
    axes[i].scatter(support_vectors[:, 0], support_vectors[:, 1],
                   s=200, facecolors='none', edgecolors='yellow', linewidth=3)

    axes[i].set_title(f'C = {C}')
    axes[i].set_xlabel(f'Support Vectors: {len(support_vectors)}')

plt.suptitle('Efecto del par√°metro C en SVM\n(C√≠rculos amarillos = Support Vectors)')
plt.tight_layout()
plt.show()

print("üéØ INTERPRETACI√ìN:")
print("‚Ä¢ C BAJO (0.1): Frontera suave, muchos support vectors, menos overfitting")
print("‚Ä¢ C ALTO (100): Frontera compleja, pocos support vectors, riesgo overfitting")
print("‚Ä¢ M√°s support vectors = Modelo m√°s simple y generalizable")
print("‚Ä¢ Menos support vectors = Modelo m√°s espec√≠fico y complejo")

# Experimento con datos ruidosos
print(f"\nüß™ EXPERIMENTO CON DATOS RUIDOSOS:")
X_noisy, y_noisy = make_classification(n_samples=200, n_features=2,
                                     n_redundant=0, n_informative=2,
                                     class_sep=0.5, flip_y=0.1,  # 10% de ruido
                                     random_state=42)

for C in [0.1, 1.0, 10.0]:
    svm = SVC(C=C, random_state=42)
    svm.fit(X_noisy, y_noisy)

    train_score = svm.score(X_noisy, y_noisy)
    n_support = len(svm.support_vectors_)

    print(f"C={C:4.1f} ‚Üí Accuracy: {train_score:.3f} | Support Vectors: {n_support:3d}")
```

#### **kernel** (Tipo de kernel)
**¬øQu√© es?** La "lente" que usa SVM para ver patrones complejos en los datos.

```python
# ANALOG√çA: Diferentes tipos de lentes para ver patrones
# 'linear': Lentes normales - solo ve l√≠neas rectas
# 'rbf': Lentes m√°gicas - puede ver c√≠rculos y curvas
# 'poly': Lentes especiales - ve patrones polinomiales

# Crear diferentes tipos de datos para probar kernels
def create_datasets():
    datasets = {}

    # Dataset 1: Linealmente separable
    X1, y1 = make_classification(n_samples=100, n_features=2, n_redundant=0,
                               n_informative=2, n_clusters_per_class=1,
                               class_sep=2.0, random_state=42)
    datasets['Lineal'] = (X1, y1)

    # Dataset 2: C√≠rculos conc√©ntricos (no lineal)
    from sklearn.datasets import make_circles
    X2, y2 = make_circles(n_samples=100, noise=0.1, factor=0.3, random_state=42)
    datasets['C√≠rculos'] = (X2, y2)

    # Dataset 3: Lunas (no lineal)
    from sklearn.datasets import make_moons
    X3, y3 = make_moons(n_samples=100, noise=0.2, random_state=42)
    datasets['Lunas'] = (X3, y3)

    return datasets

datasets = create_datasets()
kernels = ['linear', 'rbf', 'poly']

fig, axes = plt.subplots(len(datasets), len(kernels), figsize=(15, 12))

for i, (dataset_name, (X, y)) in enumerate(datasets.items()):
    for j, kernel in enumerate(kernels):
        try:
            svm = SVC(kernel=kernel, random_state=42)
            svm.fit(X, y)

            # Crear malla para visualizaci√≥n
            h = 0.02
            x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
            y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
            xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                               np.arange(y_min, y_max, h))

            Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
            Z = Z.reshape(xx.shape)

            axes[i, j].contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdBu)
            axes[i, j].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu, edgecolors='black')

            score = svm.score(X, y)
            axes[i, j].set_title(f'{dataset_name}\nKernel: {kernel}\nAccuracy: {score:.2f}')

        except Exception as e:
            axes[i, j].set_title(f'{dataset_name}\nKernel: {kernel}\nError')
            axes[i, j].text(0.5, 0.5, 'Error', ha='center', va='center', transform=axes[i, j].transAxes)

plt.tight_layout()
plt.show()

print("üîç GU√çA DE KERNELS:")
print("‚Ä¢ 'linear': Para datos linealmente separables (l√≠nea recta los separa)")
print("‚Ä¢ 'rbf': MEJOR OPCI√ìN GENERAL - funciona con la mayor√≠a de problemas")
print("‚Ä¢ 'poly': Para patrones polinomiales espec√≠ficos")
print("‚Ä¢ 'sigmoid': Raramente usado, similar a redes neuronales")

print(f"\nüí° RECOMENDACI√ìN:")
print("1. Siempre prueba 'rbf' primero")
print("2. Si es muy lento, prueba 'linear'")
print("3. Solo usa 'poly' si tienes razones espec√≠ficas")
```

#### **gamma** (Par√°metro del kernel RBF)
**¬øQu√© es?** Controla qu√© tan "curva" puede ser la frontera de decisi√≥n.

```python
# ANALOG√çA: Nivel de detalle en un mapa
# gamma BAJO: Mapa de pa√≠s - ve patrones generales, fronteras suaves
# gamma ALTO: Mapa de ciudad - ve cada detalle, fronteras muy espec√≠ficas

from sklearn.datasets import make_circles

# Usar datos circulares para ver mejor el efecto
X_circles, y_circles = make_circles(n_samples=100, noise=0.1, factor=0.3, random_state=42)

gamma_values = ['scale', 'auto', 0.1, 1.0, 10.0, 100.0]

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.ravel()

for i, gamma in enumerate(gamma_values):
    svm = SVC(kernel='rbf', gamma=gamma, random_state=42)
    svm.fit(X_circles, y_circles)

    # Visualizaci√≥n
    h = 0.02
    x_min, x_max = X_circles[:, 0].min() - 0.5, X_circles[:, 0].max() + 0.5
    y_min, y_max = X_circles[:, 1].min() - 0.5, X_circles[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                       np.arange(y_min, y_max, h))

    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    axes[i].contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdBu)
    scatter = axes[i].scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles,
                            cmap=plt.cm.RdBu, edgecolors='black')

    score = svm.score(X_circles, y_circles)
    n_support = len(svm.support_vectors_)

    axes[i].set_title(f'Œ≥ = {gamma}\nAccuracy: {score:.3f}\nSupport V.: {n_support}')

plt.tight_layout()
plt.show()

print("üéØ INTERPRETACI√ìN DE GAMMA:")
print("‚Ä¢ gamma='scale' (default): 1/(n_features √ó X.var()) - Autom√°tico")
print("‚Ä¢ gamma='auto': 1/n_features - M√°s simple")
print("‚Ä¢ gamma BAJO (0.1): Frontera suave, m√°s generalizable")
print("‚Ä¢ gamma ALTO (100): Frontera muy detallada, riesgo overfitting")

# Experimento num√©rico
print(f"\nüìä EXPERIMENTO NUM√âRICO:")
for gamma in [0.1, 1.0, 10.0]:
    svm = SVC(kernel='rbf', gamma=gamma, random_state=42)
    svm.fit(X_circles, y_circles)

    train_score = svm.score(X_circles, y_circles)
    n_support = len(svm.support_vectors_)

    print(f"Œ≥={gamma:4.1f} ‚Üí Accuracy: {train_score:.3f} | Support Vectors: {n_support:2d}")

print(f"\nüí° TIPS PARA GAMMA:")
print("‚Ä¢ Empieza con 'scale' (valor por defecto)")
print("‚Ä¢ Si hay overfitting ‚Üí reduce gamma")
print("‚Ä¢ Si hay underfitting ‚Üí aumenta gamma")
print("‚Ä¢ Para datasets grandes ‚Üí usa gamma m√°s bajo")
```

---

## Par√°metros de Evaluaci√≥n

### üìä M√©tricas de Clasificaci√≥n

#### **cross_val_score()**
```python
cross_val_score(
    estimator,               # El modelo a evaluar
    X, y,                   # Datos y etiquetas
    cv=5,                   # N√∫mero de folds
    scoring='accuracy',     # M√©trica a usar
    n_jobs=None             # Paralelizaci√≥n
)
```

**¬øQu√© hace cada par√°metro?**

```python
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

# Crear datos de ejemplo
X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)
model = RandomForestClassifier(random_state=42)

print("üîç EXPLICANDO cross_val_score:")

# cv: N√∫mero de divisiones
print(f"\nüìä Par√°metro 'cv' (cross-validation folds):")
for cv in [3, 5, 10]:
    scores = cross_val_score(model, X, y, cv=cv)
    print(f"cv={cv:2d} ‚Üí Scores: {[f'{s:.3f}' for s in scores]} | Promedio: {scores.mean():.3f}")
    print(f"      ‚Üí Se entrena {cv} veces, cada vez con {(cv-1)/cv*100:.0f}% datos para entrenar")

# scoring: Diferentes m√©tricas
print(f"\nüéØ Par√°metro 'scoring' (qu√© medir):")
scoring_options = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']

for scoring in scoring_options:
    try:
        scores = cross_val_score(model, X, y, cv=3, scoring=scoring)
        print(f"scoring='{scoring:10}' ‚Üí Promedio: {scores.mean():.3f}")
    except Exception as e:
        print(f"scoring='{scoring:10}' ‚Üí Error: {str(e)[:40]}...")

# n_jobs: Paralelizaci√≥n
print(f"\n‚ö° Par√°metro 'n_jobs' (velocidad):")
import time

for n_jobs in [1, -1]:  # 1 = un core, -1 = todos los cores
    start_time = time.time()
    scores = cross_val_score(model, X, y, cv=5, n_jobs=n_jobs)
    time_taken = time.time() - start_time

    jobs_desc = "1 core" if n_jobs == 1 else "todos los cores"
    print(f"n_jobs={n_jobs:2d} ({jobs_desc:15}) ‚Üí Tiempo: {time_taken:.3f}s | Score: {scores.mean():.3f}")
```

#### **M√©tricas individuales**
```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Crear predicciones de ejemplo
y_true = [0, 0, 1, 1, 1, 0, 1, 0]
y_pred = [0, 1, 1, 1, 0, 0, 1, 0]  # Algunas predicciones incorrectas

print("üéØ M√âTRICAS EXPLICADAS CON EJEMPLO:")
print(f"Valores reales:    {y_true}")
print(f"Predicciones:      {y_pred}")

# Calcular m√©tricas
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print(f"\nüìà RESULTADOS:")
print(f"Accuracy:  {accuracy:.3f} - ¬øQu√© % de predicciones fueron correctas?")
print(f"Precision: {precision:.3f} - De los que predije como positivos, ¬øqu√© % eran correctos?")
print(f"Recall:    {recall:.3f} - De todos los positivos reales, ¬øqu√© % detect√©?")
print(f"F1-Score:  {f1:.3f} - Balance entre Precision y Recall")

# An√°lisis manual paso a paso
print(f"\nüîç AN√ÅLISIS MANUAL:")

# Contar verdaderos/falsos positivos/negativos
tp = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 1)  # Verdaderos Positivos
fp = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1)  # Falsos Positivos
tn = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 0)  # Verdaderos Negativos
fn = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 0)  # Falsos Negativos

print(f"Verdaderos Positivos (TP): {tp} - Acert√© que era positivo")
print(f"Falsos Positivos (FP):     {fp} - Dije positivo pero era negativo")
print(f"Verdaderos Negativos (TN): {tn} - Acert√© que era negativo")
print(f"Falsos Negativos (FN):     {fn} - Dije negativo pero era positivo")

print(f"\nüßÆ C√ÅLCULOS MANUALES:")
print(f"Accuracy = (TP+TN)/(TP+TN+FP+FN) = ({tp}+{tn})/({tp}+{tn}+{fp}+{fn}) = {(tp+tn)/(tp+tn+fp+fn):.3f}")
print(f"Precision = TP/(TP+FP) = {tp}/({tp}+{fp}) = {tp/(tp+fp) if (tp+fp) > 0 else 'N/A':.3f}")
print(f"Recall = TP/(TP+FN) = {tp}/({tp}+{fn}) = {tp/(tp+fn) if (tp+fn) > 0 else 'N/A':.3f}")
```

---

## Par√°metros de Validaci√≥n Cruzada

### üîÑ KFold y StratifiedKFold

```python
from sklearn.model_selection import KFold, StratifiedKFold

KFold(
    n_splits=5,              # N√∫mero de divisiones
    shuffle=False,           # ¬øMezclar datos antes de dividir?
    random_state=None        # Semilla para reproducibilidad
)

StratifiedKFold(
    n_splits=5,              # N√∫mero de divisiones
    shuffle=False,           # ¬øMezclar datos antes de dividir?
    random_state=None        # Semilla para reproducibilidad
)
```

**Explicaci√≥n detallada:**

```python
import numpy as np
from sklearn.datasets import make_classification

# Crear datos desbalanceados para mostrar la diferencia
X, y = make_classification(n_samples=100, n_classes=3, n_informative=3,
                         weights=[0.6, 0.3, 0.1], random_state=42)

print("üìä DATOS ORIGINALES:")
unique, counts = np.unique(y, return_counts=True)
for cls, count in zip(unique, counts):
    print(f"Clase {cls}: {count:2d} muestras ({count/len(y)*100:.1f}%)")

def compare_cv_strategies():
    print(f"\nüîç COMPARANDO ESTRATEGIAS DE CV:")

    # KFold normal
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    print(f"\nüìù KFold normal:")
    for fold, (train_idx, test_idx) in enumerate(kf.split(X, y)):
        test_distribution = np.bincount(y[test_idx])
        print(f"Fold {fold+1}: Test set ‚Üí Clase 0: {test_distribution[0]:2d}, "
              f"Clase 1: {test_distribution[1]:2d}, Clase 2: {test_distribution[2]:2d}")

    # StratifiedKFold
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    print(f"\n‚öñÔ∏è  StratifiedKFold:")
    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):
        test_distribution = np.bincount(y[test_idx])
        print(f"Fold {fold+1}: Test set ‚Üí Clase 0: {test_distribution[0]:2d}, "
              f"Clase 1: {test_distribution[1]:2d}, Clase 2: {test_distribution[2]:2d}")

compare_cv_strategies()

print(f"\nüí° DIFERENCIAS CLAVE:")
print("‚Ä¢ KFold: Puede crear folds desbalanceados por casualidad")
print("‚Ä¢ StratifiedKFold: Mantiene la proporci√≥n de clases en cada fold")
print("‚Ä¢ ¬øCu√°ndo usar cada uno?")
print("  - KFold: Regresi√≥n o datos perfectamente balanceados")
print("  - StratifiedKFold: Clasificaci√≥n (especialmente datos desbalanceados)")

# Ejemplo del par√°metro shuffle
print(f"\nüîÄ EFECTO DEL PAR√ÅMETRO 'shuffle':")

# Sin shuffle
kf_no_shuffle = KFold(n_splits=3, shuffle=False)
print("Sin shuffle (shuffle=False):")
for fold, (train_idx, test_idx) in enumerate(kf_no_shuffle.split(X)):
    print(f"Fold {fold+1}: Test indices = {test_idx[:5]}...{test_idx[-5:]} (primeros y √∫ltimos 5)")

# Con shuffle
kf_shuffle = KFold(n_splits=3, shuffle=True, random_state=42)
print(f"\nCon shuffle (shuffle=True):")
for fold, (train_idx, test_idx) in enumerate(kf_shuffle.split(X)):
    print(f"Fold {fold+1}: Test indices = {test_idx[:5]}...{test_idx[-5:]} (primeros y √∫ltimos 5)")

print(f"\n‚ö†Ô∏è  IMPORTANTE:")
print("‚Ä¢ shuffle=False: Los datos se dividen en orden secuencial")
print("‚Ä¢ shuffle=True: Los datos se mezclan antes de dividir")
print("‚Ä¢ ¬øCu√°ndo usar shuffle=True? CASI SIEMPRE (evita sesgos por orden)")
```

### üéØ Par√°metros de cross_validate

```python
from sklearn.model_selection import cross_validate

cross_validate(
    estimator,                    # Modelo a evaluar
    X, y,                        # Datos
    groups=None,                 # Grupos para GroupKFold
    scoring=None,                # M√©trica(s) a calcular
    cv=None,                     # Estrategia de CV
    n_jobs=None,                 # Paralelizaci√≥n
    verbose=0,                   # Nivel de detalle en output
    fit_params=None,             # Par√°metros adicionales para fit()
    pre_dispatch='2*n_jobs',     # Control de memoria
    return_train_score=False,    # ¬øCalcular score en train tambi√©n?
    return_estimator=False,      # ¬øDevolver modelos entrenados?
    error_score=np.nan          # Qu√© devolver si hay error
)
```

**Ejemplos pr√°cticos:**

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

X, y = make_classification(n_samples=500, n_features=10, n_classes=2, random_state=42)
model = RandomForestClassifier(n_estimators=50, random_state=42)

print("üî¨ EXPLORANDO cross_validate:")

# Ejemplo b√°sico
print(f"\n1Ô∏è‚É£ Uso b√°sico:")
cv_results = cross_validate(model, X, y, cv=3)
print("Llaves disponibles:", list(cv_results.keys()))
print(f"Test scores: {cv_results['test_score']}")
print(f"Fit times: {cv_results['fit_time']}")
print(f"Score times: {cv_results['score_time']}")

# Con m√∫ltiples m√©tricas
print(f"\n2Ô∏è‚É£ Con m√∫ltiples m√©tricas:")
scoring = ['accuracy', 'precision', 'recall']
cv_results = cross_validate(model, X, y, cv=3, scoring=scoring)
for metric in scoring:
    scores = cv_results[f'test_{metric}']
    print(f"{metric:10}: {scores.mean():.3f} ¬± {scores.std():.3f}")

# Con train scores
print(f"\n3Ô∏è‚É£ Incluyendo scores de entrenamiento:")
cv_results = cross_validate(model, X, y, cv=3, scoring='accuracy',
                          return_train_score=True)
train_scores = cv_results['train_score']
test_scores = cv_results['test_score']

print("An√°lisis de overfitting:")
for i in range(len(train_scores)):
    gap = train_scores[i] - test_scores[i]
    status = "‚ö†Ô∏è Overfitting" if gap > 0.05 else "‚úÖ OK"
    print(f"Fold {i+1}: Train={train_scores[i]:.3f}, Test={test_scores[i]:.3f}, Gap={gap:.3f} {status}")

# Con verbose para ver progreso
print(f"\n4Ô∏è‚É£ Con informaci√≥n de progreso:")
print("(verbose=1 muestra progreso durante la ejecuci√≥n)")
cv_results = cross_validate(model, X, y, cv=3, verbose=1)

# Control de paralelizaci√≥n y memoria
print(f"\n5Ô∏è‚É£ Configuraci√≥n avanzada:")
print("Par√°metros √∫tiles para datasets grandes:")
print("‚Ä¢ n_jobs=-1: Usar todos los cores disponibles")
print("‚Ä¢ pre_dispatch='2*n_jobs': Controla cu√°ntos trabajos se preparan a la vez")
print("‚Ä¢ verbose=1: Muestra progreso")

example_config = """
cv_results = cross_validate(
    model, X, y,
    cv=5,
    scoring=['accuracy', 'f1'],
    n_jobs=-1,              # M√°xima paralelizaci√≥n
    verbose=1,              # Mostrar progreso
    return_train_score=True, # Para detectar overfitting
    pre_dispatch='2*n_jobs'  # Control de memoria
)
"""
print(f"Ejemplo de configuraci√≥n completa:")
print(example_config)
```

---

## Par√°metros de GridSearchCV (Continuaci√≥n)

### üîç **verbose** (Nivel de informaci√≥n)
**¬øQu√© es?** Controla cu√°nta informaci√≥n muestra GridSearchCV mientras busca.

```python
# ANALOG√çA: Niveles de comunicaci√≥n en una b√∫squeda
# verbose=0: Como un detective silencioso - no dice nada hasta el final
# verbose=1: Como un reportero - te informa cada progreso importante
# verbose=2: Como un comentarista deportivo - te cuenta todo en detalle

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

X, y = make_classification(n_samples=500, n_features=10, n_classes=2, random_state=42)

print("üì¢ NIVELES DE VERBOSIDAD EN GRIDSEARCHCV:")

param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [3, 5]
}

print("\nüîá verbose=0 (Silencioso):")
print("   ‚Üí No muestra ning√∫n progreso")
print("   ‚Üí Solo el resultado final")
print("   ‚Üí Ideal para: Scripts autom√°ticos, producci√≥n")

print("\nüì¢ verbose=1 (Informativo):")
print("   ‚Üí Muestra cada combinaci√≥n completada")
print("   ‚Üí Tiempo estimado restante")
print("   ‚Üí Ideal para: Desarrollo, experimentos interactivos")

print("\nüìª verbose=2+ (Muy detallado):")
print("   ‚Üí Informaci√≥n de cada fold individual")
print("   ‚Üí Detalles internos del proceso")
print("   ‚Üí Ideal para: Debugging, an√°lisis profundo")

# Ejemplo pr√°ctico
print("\nüß™ EJEMPLO PR√ÅCTICO:")
print("Con verbose=1, ver√≠as algo as√≠:")

example_output = """
Fitting 3 folds for each of 4 candidates, totalling 12 fits
[CV 1/3] END max_depth=3, n_estimators=50;, score=0.867, total= 0.1s
[CV 2/3] END max_depth=3, n_estimators=50;, score=0.853, total= 0.1s
[CV 3/3] END max_depth=3, n_estimators=50;, score=0.840, total= 0.1s
[CV 1/3] END max_depth=3, n_estimators=100;, score=0.873, total= 0.2s
...
"""

print(example_output)

print("üí° CU√ÅNDO USAR CADA NIVEL:")
print("‚Ä¢ verbose=0: Cuando GridSearchCV es parte de un pipeline m√°s grande")
print("‚Ä¢ verbose=1: Para experimentos normales (RECOMENDADO)")
print("‚Ä¢ verbose=2: Solo para debugging o an√°lisis muy detallado")
```

### ‚ö° **n_jobs** (Paralelizaci√≥n)
**¬øQu√© es?** Cu√°ntos procesadores usar simult√°neamente.

```python
# ANALOG√çA: Trabajadores en una f√°brica
# n_jobs=1: Un solo trabajador hace todo el trabajo secuencialmente
# n_jobs=2: Dos trabajadores dividen el trabajo
# n_jobs=-1: Todos los trabajadores disponibles colaboran

import time
import multiprocessing

print("üè≠ PARALELIZACI√ìN EN GRIDSEARCHCV:")
print(f"Tu computadora tiene {multiprocessing.cpu_count()} cores disponibles")

def time_gridsearch(n_jobs, description):
    """Medir tiempo de GridSearchCV con diferentes niveles de paralelizaci√≥n"""
    grid = GridSearchCV(
        RandomForestClassifier(random_state=42),
        param_grid={'n_estimators': [10, 20, 30], 'max_depth': [3, 5, 7]},
        cv=3,
        n_jobs=n_jobs,
        verbose=0
    )

    start_time = time.time()
    grid.fit(X, y)
    end_time = time.time()

    return end_time - start_time

print(f"\n‚è±Ô∏è COMPARACI√ìN DE TIEMPOS:")

# Probar diferentes configuraciones
configs = [
    (1, "Secuencial (1 core)"),
    (2, "Paralelo (2 cores)"),
    (-1, "M√°ximo paralelo (todos los cores)")
]

results = []
for n_jobs, description in configs:
    time_taken = time_gridsearch(n_jobs, description)
    speedup = results[0] / time_taken if results else 1.0
    results.append(time_taken)

    print(f"{description:25} ‚Üí {time_taken:.2f}s (speedup: {speedup:.1f}x)")

print(f"\nüéØ RECOMENDACIONES PARA n_jobs:")
print("‚Ä¢ n_jobs=1: Para debugging o cuando tienes poca RAM")
print("‚Ä¢ n_jobs=2-4: Equilibrio entre velocidad y estabilidad")
print("‚Ä¢ n_jobs=-1: M√°xima velocidad (RECOMENDADO para experimentos)")
print("‚Ä¢ ‚ö†Ô∏è Cuidado: M√°s jobs = m√°s RAM necesaria")

print(f"\nüíæ CONSIDERACIONES DE MEMORIA:")
print("‚Ä¢ Cada job necesita cargar el dataset completo")
print("‚Ä¢ n_jobs=4 con dataset de 1GB ‚Üí necesitas ~4GB RAM")
print("‚Ä¢ Si tu computadora se queda lenta ‚Üí reduce n_jobs")

# Ejemplo de configuraci√≥n adaptativa
print(f"\nü§ñ CONFIGURACI√ìN INTELIGENTE:")
smart_config = """
import multiprocessing

# Configuraci√≥n que se adapta a tu computadora
n_cores = multiprocessing.cpu_count()

if dataset_size_gb < 1.0:
    n_jobs = -1  # Usar todos los cores
elif dataset_size_gb < 5.0:
    n_jobs = min(4, n_cores)  # M√°ximo 4 cores
else:
    n_jobs = 2  # Solo 2 cores para datasets grandes

grid = GridSearchCV(model, params, n_jobs=n_jobs)
"""
print(smart_config)
```

### üéØ **scoring** (M√∫ltiples m√©tricas)
**¬øQu√© es?** Qu√© m√©tricas usar para evaluar cada combinaci√≥n de par√°metros.

```python
# ANALOG√çA: Criterios para evaluar estudiantes
# Una sola m√©trica: Solo la nota del examen final
# M√∫ltiples m√©tricas: Examen + tareas + participaci√≥n + asistencia

from sklearn.datasets import make_classification

# Crear dataset con clases desbalanceadas para mostrar diferencias entre m√©tricas
X_imb, y_imb = make_classification(n_samples=1000, n_classes=2, weights=[0.9, 0.1],
                                  n_features=10, random_state=42)

print("üìä M√öLTIPLES M√âTRICAS EN GRIDSEARCHCV:")
print(f"Dataset desbalanceado: Clase 0: {sum(y_imb==0)} muestras, Clase 1: {sum(y_imb==1)} muestras")

# Definir m√∫ltiples m√©tricas
scoring_metrics = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1': 'f1',
    'roc_auc': 'roc_auc'
}

print(f"\nüéØ CONFIGURACI√ìN DE M√öLTIPLES M√âTRICAS:")
multi_metric_example = """
# Opci√≥n 1: Lista de strings
scoring = ['accuracy', 'precision', 'recall', 'f1']

# Opci√≥n 2: Diccionario (m√°s claro)
scoring = {
    'acc': 'accuracy',
    'prec': 'precision',
    'rec': 'recall',
    'f1': 'f1'
}

grid = GridSearchCV(model, param_grid, scoring=scoring, refit='f1')
"""
print(multi_metric_example)

# Ejemplo pr√°ctico
param_grid_simple = {'n_estimators': [10, 50, 100]}

grid_multi = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid_simple,
    scoring=scoring_metrics,
    cv=3,
    refit='f1',  # Usar F1 para seleccionar el mejor modelo
    return_train_score=True
)

grid_multi.fit(X_imb, y_imb)

print(f"\nüìà RESULTADOS CON M√öLTIPLES M√âTRICAS:")
results_df = pd.DataFrame(grid_multi.cv_results_)

# Mostrar las m√©tricas para cada configuraci√≥n
for idx, n_est in enumerate([10, 50, 100]):
    print(f"\nn_estimators = {n_est}:")
    for metric_name in scoring_metrics.keys():
        mean_score = results_df.loc[idx, f'mean_test_{metric_name}']
        std_score = results_df.loc[idx, f'std_test_{metric_name}']
        print(f"  {metric_name:10}: {mean_score:.3f} ¬± {std_score:.3f}")

print(f"\nüèÜ MEJOR MODELO SEG√öN DIFERENTES CRITERIOS:")
for metric_name in scoring_metrics.keys():
    best_idx = results_df[f'mean_test_{metric_name}'].idxmax()
    best_params = results_df.loc[best_idx, 'params']
    best_score = results_df.loc[best_idx, f'mean_test_{metric_name}']
    print(f"{metric_name:10}: {best_params} (score: {best_score:.3f})")

print(f"\nüí° PAR√ÅMETRO 'refit':")
print("‚Ä¢ refit='accuracy': Selecciona el mejor seg√∫n accuracy")
print("‚Ä¢ refit='f1': Selecciona el mejor seg√∫n F1 (BUENO para datos desbalanceados)")
print("‚Ä¢ refit=False: No entrena modelo final, solo eval√∫a")

print(f"\nüéØ ESTRATEGIAS DE SELECCI√ìN:")
print("‚Ä¢ Para datos balanceados: refit='accuracy'")
print("‚Ä¢ Para datos desbalanceados: refit='f1' o refit='roc_auc'")
print("‚Ä¢ Para detecci√≥n de fraude: refit='recall' (no queremos perder positivos)")
print("‚Ä¢ Para spam detection: refit='precision' (no queremos muchas falsas alarmas)")
```

### üîÑ **cross_validation strategies** (Estrategias de CV)
**¬øQu√© es?** C√≥mo dividir los datos para validaci√≥n cruzada.

```python
# ANALOG√çA: Formas de evaluar un estudiante
# KFold: 5 ex√°menes sorpresa aleatorios
# StratifiedKFold: 5 ex√°menes que cubren todos los temas proporcionalmente
# TimeSeriesSplit: Ex√°menes cronol√≥gicos (no puedes saber el futuro)
# GroupKFold: Ex√°menes por escuelas (no mezclar grupos relacionados)

from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit, GroupKFold
import numpy as np

print("üîÑ ESTRATEGIAS DE VALIDACI√ìN CRUZADA:")

# Crear diferentes tipos de datos
X_regular, y_regular = make_classification(n_samples=100, n_classes=2, random_state=42)

# Datos de series temporales simulados
X_time = np.random.randn(100, 5)
y_time = np.random.randint(0, 2, 100)
dates = pd.date_range('2020-01-01', periods=100, freq='D')

# Datos con grupos
groups = np.repeat([1, 2, 3, 4, 5], 20)  # 5 grupos de 20 muestras cada uno

print(f"\n1Ô∏è‚É£ KFold (Validaci√≥n cruzada est√°ndar):")
kf = KFold(n_splits=5, shuffle=True, random_state=42)

print("Caracter√≠sticas:")
print("‚Ä¢ Divide datos aleatoriamente en k grupos")
print("‚Ä¢ Cada grupo se usa como test una vez")
print("‚Ä¢ Ignora las clases ‚Üí puede crear folds desbalanceados")

print("¬øCu√°ndo usar?")
print("‚Ä¢ Problemas de regresi√≥n")
print("‚Ä¢ Datos perfectamente balanceados")
print("‚Ä¢ Cuando el orden de los datos no importa")

print(f"\n2Ô∏è‚É£ StratifiedKFold (Mantiene proporciones):")
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

print("Caracter√≠sticas:")
print("‚Ä¢ Mantiene la proporci√≥n de clases en cada fold")
print("‚Ä¢ Cada fold es representativo del dataset completo")
print("‚Ä¢ Reduce varianza entre folds")

# Demostraci√≥n pr√°ctica
y_imbalanced = np.array([0]*80 + [1]*20)  # 80% clase 0, 20% clase 1
print(f"\nEjemplo con datos desbalanceados (80% vs 20%):")

print("KFold regular:")
for fold, (train_idx, test_idx) in enumerate(KFold(n_splits=5, shuffle=True, random_state=42).split(y_imbalanced)):
    test_dist = np.bincount(y_imbalanced[test_idx])
    print(f"  Fold {fold+1}: {test_dist[0]:2d} vs {test_dist[1]:2d} ({test_dist[1]/(test_dist[0]+test_dist[1])*100:.0f}% clase 1)")

print("StratifiedKFold:")
for fold, (train_idx, test_idx) in enumerate(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(range(len(y_imbalanced)), y_imbalanced)):
    test_dist = np.bincount(y_imbalanced[test_idx])
    print(f"  Fold {fold+1}: {test_dist[0]:2d} vs {test_dist[1]:2d} ({test_dist[1]/(test_dist[0]+test_dist[1])*100:.0f}% clase 1)")

print("¬øCu√°ndo usar StratifiedKFold?")
print("‚Ä¢ SIEMPRE en problemas de clasificaci√≥n")
print("‚Ä¢ Especialmente con datos desbalanceados")
print("‚Ä¢ Cuando quieres resultados m√°s estables")

print(f"\n3Ô∏è‚É£ TimeSeriesSplit (Para series temporales):")
tss = TimeSeriesSplit(n_splits=5)

print("Caracter√≠sticas:")
print("‚Ä¢ Respeta el orden cronol√≥gico")
print("‚Ä¢ El conjunto de entrenamiento siempre es anterior al de test")
print("‚Ä¢ Cada fold tiene m√°s datos de entrenamiento que el anterior")

print("Visualizaci√≥n de TimeSeriesSplit:")
print("Fold 1: Train [1---10] ‚Üí Test [11-15]")
print("Fold 2: Train [1------15] ‚Üí Test [16-20]")
print("Fold 3: Train [1---------20] ‚Üí Test [21-25]")
print("Fold 4: Train [1------------25] ‚Üí Test [26-30]")
print("Fold 5: Train [1---------------30] ‚Üí Test [31-35]")

print("¬øCu√°ndo usar?")
print("‚Ä¢ Datos de series temporales (precios, ventas, clima)")
print("‚Ä¢ Cuando el orden temporal importa")
print("‚Ä¢ Predicciones donde no puedes 'ver el futuro'")

print(f"\n4Ô∏è‚É£ GroupKFold (Para datos agrupados):")
gkf = GroupKFold(n_splits=3)

print("Caracter√≠sticas:")
print("‚Ä¢ Asegura que grupos relacionados no se mezclen")
print("‚Ä¢ Todos los datos de un grupo van al mismo fold")
print("‚Ä¢ Previene data leakage entre grupos relacionados")

print("Ejemplo de grupos:")
print("‚Ä¢ Pacientes: Cada persona puede tener m√∫ltiples mediciones")
print("‚Ä¢ Escuelas: M√∫ltiples estudiantes por escuela")
print("‚Ä¢ Empresas: M√∫ltiples transacciones por empresa")

groups_example = [1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5]
print(f"\nDatos con grupos: {groups_example}")
print("GroupKFold asegura que:")
print("‚Ä¢ Todos los datos del grupo 1 van juntos")
print("‚Ä¢ No hay 'leakage' entre entrenar con grupo 1 y testear con grupo 1")

print("¬øCu√°ndo usar?")
print("‚Ä¢ Datos m√©dicos (m√∫ltiples mediciones por paciente)")
print("‚Ä¢ Datos financieros (m√∫ltiples transacciones por cliente)")
print("‚Ä¢ Cualquier situaci√≥n con dependencias naturales")

# Configuraci√≥n en GridSearchCV
print(f"\n‚öôÔ∏è CONFIGURACI√ìN EN GRIDSEARCHCV:")
config_examples = """
# Est√°ndar (recomendado para la mayor√≠a)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Series temporales
cv = TimeSeriesSplit(n_splits=5)

# Datos agrupados
cv = GroupKFold(n_splits=3)

# Usar en GridSearchCV
grid = GridSearchCV(model, param_grid, cv=cv)
"""
print(config_examples)
```

## Par√°metros Avanzados de Preprocesamiento

### üé® **StandardScaler** (Estandarizaci√≥n)
**¬øQu√© es?** Transforma los datos para que tengan media 0 y desviaci√≥n est√°ndar 1.

```python
# ANALOG√çA: Traducir diferentes idiomas a un idioma com√∫n
# Imagina que tienes datos en diferentes "idiomas":
# - Edad: 25, 30, 35 (n√∫meros peque√±os)
# - Salario: 50000, 60000, 70000 (n√∫meros grandes)
# - Altura: 1.70, 1.80, 1.90 (n√∫meros decimales)
#
# StandardScaler los "traduce" a un idioma com√∫n donde todos
# tienen la misma importancia y escala

from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt

print("üé® STANDARDSCALER: NORMALIZANDO DATOS")

# Crear datos con diferentes escalas
np.random.seed(42)
datos_originales = {
    'Edad': np.random.normal(35, 10, 100),        # Media~35, rango 15-55
    'Salario': np.random.normal(60000, 15000, 100), # Media~60k, rango 30k-90k
    'Altura': np.random.normal(1.70, 0.15, 100)   # Media~1.70, rango 1.4-2.0
}

print("üìä DATOS ORIGINALES (antes de estandarizar):")
for variable, valores in datos_originales.items():
    print(f"{variable:8}: Media={valores.mean():8.2f}, Std={valores.std():6.2f}, "
          f"Rango=[{valores.min():6.2f}, {valores.max():6.2f}]")

# Problema: Los algoritmos se "confunden" con escalas diferentes
print(f"\n‚ùó PROBLEMA SIN ESTANDARIZACI√ìN:")
print("‚Ä¢ Los algoritmos piensan que 'Salario' es 1000x m√°s importante que 'Altura'")
print("‚Ä¢ KNN calcula distancias incorrectas: diferencia de 10k en salario >> diferencia de 0.1m en altura")
print("‚Ä¢ SVM y regresi√≥n log√≠stica convergen mal")
print("‚Ä¢ Los gradientes son inestables")

# Aplicar StandardScaler
X_original = np.column_stack([datos_originales['Edad'],
                            datos_originales['Salario'],
                            datos_originales['Altura']])

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_original)

print(f"\nüìà DATOS DESPU√âS DE ESTANDARIZACI√ìN:")
variables = ['Edad', 'Salario', 'Altura']
for i, variable in enumerate(variables):
    valores_scaled = X_scaled[:, i]
    print(f"{variable:8}: Media={valores_scaled.mean():8.2f}, Std={valores_scaled.std():6.2f}, "
          f"Rango=[{valores_scaled.min():6.2f}, {valores_scaled.max():6.2f}]")

print(f"\n‚úÖ BENEFICIOS DESPU√âS DE ESTANDARIZACI√ìN:")
print("‚Ä¢ Todas las variables tienen la misma importancia inicial")
print("‚Ä¢ Media = 0, Desviaci√≥n est√°ndar = 1 para todas")
print("‚Ä¢ Los algoritmos convergen m√°s r√°pido y estable")
print("‚Ä¢ Las distancias se calculan correctamente")

# Visualizaci√≥n
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Datos originales
ax1.boxplot([datos_originales['Edad'], datos_originales['Salario']/1000, datos_originales['Altura']],
           labels=['Edad', 'Salario (k‚Ç¨)', 'Altura'])
ax1.set_title('Datos Originales\n(Escalas muy diferentes)')
ax1.set_ylabel('Valores')

# Datos estandarizados
ax2.boxplot([X_scaled[:, 0], X_scaled[:, 1], X_scaled[:, 2]],
           labels=['Edad', 'Salario', 'Altura'])
ax2.set_title('Datos Estandarizados\n(Misma escala)')
ax2.set_ylabel('Valores estandarizados')
ax2.axhline(y=0, color='red', linestyle='--', alpha=0.7, label='Media = 0')

plt.tight_layout()
plt.show()

print(f"\nüîç ¬øC√ìMO FUNCIONA INTERNAMENTE?")
print("Formula: z = (x - media) / desviaci√≥n_est√°ndar")
print("\nEjemplo manual:")
edad_ejemplo = 45
media_edad = datos_originales['Edad'].mean()
std_edad = datos_originales['Edad'].std()
edad_estandarizada = (edad_ejemplo - media_edad) / std_edad

print(f"Edad original: {edad_ejemplo}")
print(f"Media de edades: {media_edad:.2f}")
print(f"Std de edades: {std_edad:.2f}")
print(f"Edad estandarizada: ({edad_ejemplo} - {media_edad:.2f}) / {std_edad:.2f} = {edad_estandarizada:.2f}")

print(f"\nüéØ PAR√ÅMETROS DE STANDARDSCALER:")
scaler_params = """
StandardScaler(
    copy=True,           # ¬øCrear copia o modificar original?
    with_mean=True,      # ¬øCentrar en la media (restar media)?
    with_std=True        # ¬øEscalar por desviaci√≥n est√°ndar?
)
"""
print(scaler_params)

print("‚Ä¢ copy=True: SIEMPRE usar (no modifica datos originales)")
print("‚Ä¢ with_mean=True: SIEMPRE usar (centra en 0)")
print("‚Ä¢ with_std=True: SIEMPRE usar (escala por std)")

print(f"\n‚ö†Ô∏è IMPORTANTE - FIT vs TRANSFORM:")
print("‚Ä¢ fit(): Aprende la media y std de los datos de ENTRENAMIENTO")
print("‚Ä¢ transform(): Aplica la transformaci√≥n usando esos valores")
print("‚Ä¢ fit_transform(): Hace ambos pasos juntos")

fit_transform_example = """
# ‚úÖ CORRECTO
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Aprende Y transforma
X_test_scaled = scaler.transform(X_test)        # Solo transforma (usa par√°metros de train)

# ‚ùå INCORRECTO
scaler_train = StandardScaler()
scaler_test = StandardScaler()
X_train_scaled = scaler_train.fit_transform(X_train)
X_test_scaled = scaler_test.fit_transform(X_test)  # ¬°Error! Usa diferentes par√°metros
"""
print(fit_transform_example)

print(f"\nü§ñ ¬øCU√ÅNDO USAR STANDARDSCALER?")
print("‚úÖ SIEMPRE usar con:")
print("‚Ä¢ SVM (Support Vector Machine)")
print("‚Ä¢ Regresi√≥n Log√≠stica")
print("‚Ä¢ KNN (K-Nearest Neighbors)")
print("‚Ä¢ Redes Neuronales")
print("‚Ä¢ PCA (Principal Component Analysis)")

print("\n‚ùå NO necesario con:")
print("‚Ä¢ Random Forest")
print("‚Ä¢ Decision Trees")
print("‚Ä¢ Gradient Boosting")
print("(Estos algoritmos son invariantes a la escala)")

# Demostraci√≥n pr√°ctica del impacto
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score

print(f"\nüß™ EXPERIMENTO: IMPACTO EN RENDIMIENTO")

# Crear dataset con escalas muy diferentes
X_multi_scale = np.column_stack([
    np.random.normal(0, 1, 200),        # Variable normal
    np.random.normal(0, 1000, 200)     # Variable 1000x m√°s grande
])
y_binary = np.random.randint(0, 2, 200)

# SVM sin estandarizar
svm_no_scale = SVC(random_state=42)
scores_no_scale = cross_val_score(svm_no_scale, X_multi_scale, y_binary, cv=3)

# SVM con estandarizaci√≥n
svm_with_scale = SVC(random_state=42)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_multi_scale)
scores_with_scale = cross_val_score(svm_with_scale, X_scaled, y_binary, cv=3)

print(f"SVM sin estandarizar: {scores_no_scale.mean():.3f} ¬± {scores_no_scale.std():.3f}")
print(f"SVM con estandarizar: {scores_with_scale.mean():.3f} ¬± {scores_with_scale.std():.3f}")
print(f"Mejora: {scores_with_scale.mean() - scores_no_scale.mean():.3f}")
```

### üè∑Ô∏è **LabelEncoder** (Codificaci√≥n de etiquetas)
**¬øQu√© es?** Convierte categor√≠as de texto en n√∫meros.

```python
# ANALOG√çA: Asignar n√∫meros a colores
# En lugar de decir "rojo", "azul", "verde"
# Le das al computador 0, 1, 2
# Es como hacer un diccionario: rojo=0, azul=1, verde=2

from sklearn.preprocessing import LabelEncoder
import pandas as pd

print("üè∑Ô∏è LABELENCODER: CONVIRTIENDO TEXTO EN N√öMEROS")

# Ejemplo con datos categ√≥ricos
categorias_ejemplo = ['perro', 'gato', 'perro', 'p√°jaro', 'gato', 'perro', 'p√°jaro']

print(f"üìù DATOS ORIGINALES: {categorias_ejemplo}")

# Aplicar LabelEncoder
le = LabelEncoder()
categorias_codificadas = le.fit_transform(categorias_ejemplo)

print(f"üî¢ DATOS CODIFICADOS: {list(categorias_codificadas)}")
print(f"üìñ DICCIONARIO DE MAPEO:")
for i, categoria in enumerate(le.classes_):
    print(f"   {categoria} ‚Üí {i}")

print(f"\nüîÑ PROCESO INVERSO:")
# Convertir n√∫meros de vuelta a texto
categorias_decodificadas = le.inverse_transform(categorias_codificadas)
print(f"De n√∫meros a texto: {list(categorias_decodificadas)}")

print(f"\nüìä EJEMPLO PR√ÅCTICO: DATASET DE EMPLEADOS")
# Crear un dataset m√°s realista
empleados_df = pd.DataFrame({
    'Nombre': ['Ana', 'Luis', 'Mar√≠a', 'Carlos', 'Sof√≠a'],
    'Departamento': ['Ventas', 'IT', 'Ventas', 'Marketing', 'IT'],
    'Nivel': ['Junior', 'Senior', 'Mid', 'Senior', 'Junior'],
    'Salario': [30000, 70000, 45000, 60000, 35000]
})

print("Dataset original:")
print(empleados_df)

# Codificar variables categ√≥ricas
le_dept = LabelEncoder()
le_nivel = LabelEncoder()

empleados_df['Departamento_cod'] = le_dept.fit_transform(empleados_df['Departamento'])
empleados_df['Nivel_cod'] = le_nivel.fit_transform(empleados_df['Nivel'])

print(f"\nDataset con codificaci√≥n:")
print(empleados_df[['Departamento', 'Departamento_cod', 'Nivel', 'Nivel_cod']])

print(f"\nüìö DICCIONARIOS DE MAPEO:")
print("Departamentos:", dict(zip(le_dept.classes_, range(len(le_dept.classes_)))))
print("Niveles:", dict(zip(le_nivel.classes_, range(len(le_nivel.classes_)))))

print(f"\n‚ö†Ô∏è PROBLEMA CON LABELENCODER:")
print("‚Ä¢ Asigna n√∫meros arbitrarios: Junior=0, Mid=1, Senior=2")
print("‚Ä¢ El algoritmo puede pensar que Senior (2) es 'mayor' que Junior (0)")
print("‚Ä¢ Crea relaciones ordinales falsas")
print("‚Ä¢ Ejemplo: El algoritmo podr√≠a pensar que Mid est√° 'entre' Junior y Senior")

print(f"\n‚úÖ CU√ÅNDO USAR LABELENCODER:")
print("‚Ä¢ SOLO para la variable objetivo (y) en clasificaci√≥n")
print("‚Ä¢ Variables con orden natural: ['Bajo', 'Medio', 'Alto'] ‚Üí [0, 1, 2]")
print("‚Ä¢ Nunca para variables categ√≥ricas nominales como caracter√≠sticas")

print(f"\n‚ùå CU√ÅNDO NO USAR:")
print("‚Ä¢ Variables categ√≥ricas sin orden: ['Rojo', 'Azul', 'Verde']")
print("‚Ä¢ Departamentos, ciudades, marcas, etc.")
print("‚Ä¢ Para estas usar OneHotEncoder en su lugar")

# Ejemplo de problema
print(f"\nüß™ DEMOSTRACI√ìN DEL PROBLEMA:")
colores = ['rojo', 'azul', 'verde', 'rojo', 'verde']
le_color = LabelEncoder()
colores_cod = le_color.fit_transform(colores)

print(f"Colores originales: {colores}")
print(f"Colores codificados: {list(colores_cod)}")
print(f"Mapeo: {dict(zip(le_color.classes_, range(len(le_color.classes_))))}")

print(f"\n‚ùó PROBLEMA:")
print("‚Ä¢ azul=0, rojo=1, verde=2")
print("‚Ä¢ El algoritmo piensa que verde (2) est√° 'm√°s lejos' de azul (0) que rojo (1)")
print("‚Ä¢ Pero en realidad no hay relaci√≥n de orden entre colores!")

# Mejor alternativa
print(f"\nüí° ALTERNATIVA: ONEHOTENCODER")
from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False)
colores_reshaped = np.array(colores).reshape(-1, 1)
colores_onehot = ohe.fit_transform(colores_reshaped)

print("Con OneHotEncoder:")
print("Caracter√≠sticas:", ohe.get_feature_names_out(['color']))
print("Matriz one-hot:")
for i, color in enumerate(colores):
    print(f"{color:6} ‚Üí {colores_onehot[i]}")

print(f"\n‚úÖ VENTAJAS DE ONEHOT:")
print("‚Ä¢ No asume relaciones ordinales falsas")
print("‚Ä¢ Cada categor√≠a es independiente")
print("‚Ä¢ Mejor para la mayor√≠a de variables categ√≥ricas")
```

### üîÑ **OneHotEncoder** (Codificaci√≥n One-Hot)
**¬øQu√© es?** Convierte categor√≠as en columnas binarias (0 o 1).

```python
# ANALOG√çA: Interruptores de luz
# En lugar de tener un dial con 3 posiciones (1, 2, 3)
# Tienes 3 interruptores independientes:
# - Interruptor_A: ON/OFF
# - Interruptor_B: ON/OFF
# - Interruptor_C: ON/OFF
# Solo uno puede estar ON a la vez

from sklearn.preprocessing import OneHotEncoder
import pandas as pd

print("üîÑ ONEHOTENCODER: CREANDO COLUMNAS BINARIAS")

# Ejemplo b√°sico
transportes = ['coche', 'bici', 'metro', 'coche', 'bici']
print(f"üìù TRANSPORTE ORIGINAL: {transportes}")

# Aplicar OneHotEncoder
ohe = OneHotEncoder(sparse_output=False)  # sparse=False para obtener array normal
transportes_reshaped = np.array(transportes).reshape(-1, 1)
transportes_onehot = ohe.fit_transform(transportes_reshaped)

print(f"üî¢ MATRIZ ONE-HOT:")
feature_names = ohe.get_feature_names_out(['transporte'])
df_onehot = pd.DataFrame(transportes_onehot, columns=feature_names)
df_onehot['original'] = transportes
print(df_onehot)

print(f"\nüìñ INTERPRETACI√ìN:")
print("‚Ä¢ transporte_bici = 1 significa 'usa bicicleta'")
print("‚Ä¢ transporte_coche = 1 significa 'usa coche'")
print("‚Ä¢ Solo una columna puede ser 1 por fila")
print("‚Ä¢ Es como tener preguntas S√≠/No independientes")

print(f"\nüè¢ EJEMPLO REALISTA: INFORMACI√ìN DE EMPLEADOS")
empleados_data = {
    'ciudad': ['Madrid', 'Barcelona', 'Valencia', 'Madrid', 'Barcelona'],
    'departamento': ['IT', 'Ventas', 'IT', 'RRHH', 'Ventas'],
    'experiencia': ['Junior', 'Senior', 'Mid', 'Senior', 'Junior']
}

df_empleados = pd.DataFrame(empleados_data)
print("Dataset original:")
print(df_empleados)

# Aplicar OneHotEncoder a m√∫ltiples columnas
ohe_multi = OneHotEncoder(sparse_output=False)
columnas_categoricas = ['ciudad', 'departamento', 'experiencia']

# Concatenar todas las columnas categ√≥ricas
X_categorical = df_empleados[columnas_categoricas]
X_onehot = ohe_multi.fit_transform(X_categorical)

# Crear DataFrame con nombres de columnas descriptivos
feature_names_multi = ohe_multi.get_feature_names_out(columnas_categoricas)
df_onehot_multi = pd.DataFrame(X_onehot, columns=feature_names_multi)

print(f"\nDataset despu√©s de One-Hot Encoding:")
print(df_onehot_multi)

print(f"\nüîç AN√ÅLISIS DE LA TRANSFORMACI√ìN:")
print(f"‚Ä¢ Columnas originales: {len(columnas_categoricas)}")
print(f"‚Ä¢ Columnas despu√©s de OHE: {len(feature_names_multi)}")
print(f"‚Ä¢ Expansi√≥n: {len(feature_names_multi)} columnas para representar {len(columnas_categoricas)} variables")

print(f"\nüìä DESGLOSE POR VARIABLE:")
for col in columnas_categoricas:
    valores_unicos = df_empleados[col].nunique()
    cols_creadas = [name for name in feature_names_multi if name.startswith(col)]
    print(f"‚Ä¢ {col}: {valores_unicos} valores √∫nicos ‚Üí {len(cols_creadas)} columnas")

print(f"\n‚öôÔ∏è PAR√ÅMETROS IMPORTANTES DE ONEHOTENCODER:")
ohe_params = """
OneHotEncoder(
    categories='auto',        # ¬øQu√© categor√≠as incluir?
    drop=None,               # ¬øEliminar alguna columna? (para evitar multicolinealidad)
    sparse_output=True,      # ¬øDevolver matriz dispersa?
    dtype=np.float64,        # Tipo de datos de salida
    handle_unknown='error',  # ¬øQu√© hacer con categor√≠as no vistas?
    min_frequency=None,      # ¬øAgrupar categor√≠as raras?
    max_categories=None      # ¬øL√≠mite m√°ximo de categor√≠as?
)
"""
print(ohe_params)

print(f"\nüéØ PAR√ÅMETROS EXPLICADOS:")

# drop parameter
print("1Ô∏è‚É£ PAR√ÅMETRO 'drop' (Evitar multicolinealidad):")
print("‚Ä¢ drop=None: Mantiene todas las columnas")
print("‚Ä¢ drop='first': Elimina la primera categor√≠a de cada variable")
print("‚Ä¢ drop='if_binary': Solo elimina si hay exactamente 2 categor√≠as")

# Demostraci√≥n de multicolinealidad
print(f"\nProblema de multicolinealidad:")
print("Si tengo: ciudad_Madrid=1, ciudad_Barcelona=0, ciudad_Valencia=0")
print("Entonces: ciudad_Madrid = 1 - ciudad_Barcelona - ciudad_Valencia")
print("¬°Una columna es redundante!")

ohe_drop = OneHotEncoder(sparse_output=False, drop='first')
X_onehot_drop = ohe_drop.fit_transform(X_categorical)
feature_names_drop = ohe_drop.get_feature_names_out(columnas_categoricas)

print(f"\nCon drop='first':")
print(f"Columnas: {list(feature_names_drop)}")
print(f"Se eliminaron: las primeras categor√≠as de cada variable")

# handle_unknown parameter
print(f"\n2Ô∏è‚É£ PAR√ÅMETRO 'handle_unknown':")
print("‚Ä¢ 'error': Falla si ve una categor√≠a nueva (DEFAULT)")
print("‚Ä¢ 'ignore': Ignora categor√≠as nuevas (todas las columnas = 0)")
print("‚Ä¢ 'infrequent_if_exist': Usa categor√≠a 'infrequent' si existe")

# Ejemplo con categor√≠a nueva
ohe_ignore = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
ohe_ignore.fit(transportes_reshaped)

# Probar con categor√≠a nueva
nuevo_transporte = np.array(['avi√≥n']).reshape(-1, 1)  # Categor√≠a no vista
resultado_ignore = ohe_ignore.transform(nuevo_transporte)
print(f"\nCategor√≠a nueva 'avi√≥n' con handle_unknown='ignore':")
print(f"Resultado: {resultado_ignore[0]} (todas las columnas en 0)")

print(f"\n3Ô∏è‚É£ PAR√ÅMETRO 'sparse_output':")
print("‚Ä¢ True: Matriz dispersa (ahorra memoria)")
print("‚Ä¢ False: Matriz normal (m√°s f√°cil de leer)")

# Comparaci√≥n de memoria
from scipy import sparse
X_sparse = ohe_multi.set_params(sparse_output=True).fit_transform(X_categorical)
X_dense = ohe_multi.set_params(sparse_output=False).fit_transform(X_categorical)

print(f"\nComparaci√≥n de memoria:")
print(f"‚Ä¢ Matriz dispersa: {X_sparse.data.nbytes} bytes")
print(f"‚Ä¢ Matriz densa: {X_dense.nbytes} bytes")
print(f"‚Ä¢ Diferencia: {(X_dense.nbytes - X_sparse.data.nbytes) / X_dense.nbytes * 100:.1f}% menos memoria con sparse")

print(f"\nüöÄ MEJORES PR√ÅCTICAS:")
print("‚úÖ HACER:")
print("‚Ä¢ Usar sparse_output=True para datasets grandes")
print("‚Ä¢ Usar drop='first' para evitar multicolinealidad en regresi√≥n lineal")
print("‚Ä¢ Usar handle_unknown='ignore' para modelos en producci√≥n")
print("‚Ä¢ Combinar con Pipeline para automatizar")

print("‚ùå EVITAR:")
print("‚Ä¢ OneHot con variables de muy alta cardinalidad (>50 categor√≠as)")
print("‚Ä¢ Aplicar a variables ya num√©ricas")
print("‚Ä¢ Olvidar el par√°metro drop en regresiones lineales")

# Ejemplo de Pipeline
print(f"\nüîß EJEMPLO CON PIPELINE:")
pipeline_example = """
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

# Definir qu√© columnas transformar
categorical_features = ['ciudad', 'departamento', 'experiencia']
numeric_features = ['salario', 'a√±os_experiencia']

# Crear transformador
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)
    ]
)

# Pipeline completo
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression())
])

# Usar pipeline
pipeline.fit(X_train, y_train)
predictions = pipeline.predict(X_test)
"""
print(pipeline_example)
```

### üìè **MinMaxScaler** (Escalado Min-Max)
**¬øQu√© es?** Reescala datos al rango [0, 1] o a cualquier rango especificado.

```python
# ANALOG√çA: Convertir calificaciones de diferentes escalas
# Imagina estudiantes de diferentes pa√≠ses:
# - Espa√±a: notas de 0 a 10
# - Estados Unidos: notas de 0 a 100
# - Francia: notas de 0 a 20
#
# MinMaxScaler los convierte a todos a la misma escala (ej: 0 a 1)
# As√≠ 10/10 (Espa√±a) = 100/100 (EE.UU.) = 20/20 (Francia) = 1.0

from sklearn.preprocessing import MinMaxScaler
import numpy as np
import matplotlib.pyplot as plt

print("üìè MINMAXSCALER: ESCALANDO AL RANGO [0,1]")

# Crear datos con diferentes rangos
np.random.seed(42)
datos_diferentes_escalas = {
    'Edad': np.random.uniform(18, 65, 100),           # Rango: 18-65
    'Salario': np.random.uniform(25000, 85000, 100),  # Rango: 25k-85k
    'Nota_examen': np.random.uniform(3.5, 9.8, 100)  # Rango: 3.5-9.8
}

print("üìä DATOS ORIGINALES:")
for variable, valores in datos_diferentes_escalas.items():
    print(f"{variable:12}: Rango=[{valores.min():8.2f}, {valores.max():8.2f}], "
          f"Media={valores.mean():8.2f}")

# Aplicar MinMaxScaler
X_original = np.column_stack(list(datos_diferentes_escalas.values()))
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_original)

print(f"\nüìà DATOS DESPU√âS DE MIN-MAX SCALING:")
variables = ['Edad', 'Salario', 'Nota_examen']
for i, variable in enumerate(variables):
    valores_scaled = X_scaled[:, i]
    print(f"{variable:12}: Rango=[{valores_scaled.min():8.2f}, {valores_scaled.max():8.2f}], "
          f"Media={valores_scaled.mean():8.2f}")

print(f"\nüîç ¬øC√ìMO FUNCIONA LA F√ìRMULA?")
print("Formula: X_scaled = (X - X_min) / (X_max - X_min)")
print("\nEjemplo manual con la edad:")
edad_ejemplo = 45
edad_min = datos_diferentes_escalas['Edad'].min()
edad_max = datos_diferentes_escalas['Edad'].max()
edad_scaled = (edad_ejemplo - edad_min) / (edad_max - edad_min)

print(f"Edad original: {edad_ejemplo}")
print(f"Edad m√≠nima: {edad_min:.2f}")
print(f"Edad m√°xima: {edad_max:.2f}")
print(f"Edad escalada: ({edad_ejemplo} - {edad_min:.2f}) / ({edad_max:.2f} - {edad_min:.2f}) = {edad_scaled:.3f}")

# Visualizaci√≥n comparativa
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Datos originales
ax1.boxplot([datos_diferentes_escalas['Edad'],
            datos_diferentes_escalas['Salario']/1000,  # Dividir por 1000 para visualizar mejor
            datos_diferentes_escalas['Nota_examen']],
           labels=['Edad', 'Salario (k‚Ç¨)', 'Nota'])
ax1.set_title('Datos Originales\n(Diferentes escalas)')
ax1.set_ylabel('Valores')

# Datos escalados
ax2.boxplot([X_scaled[:, 0], X_scaled[:, 1], X_scaled[:, 2]],
           labels=['Edad', 'Salario', 'Nota'])
ax2.set_title('Datos Escalados con MinMaxScaler\n(Rango [0,1])')
ax2.set_ylabel('Valores escalados')
ax2.axhline(y=0, color='red', linestyle='--', alpha=0.7, label='Min = 0')
ax2.axhline(y=1, color='red', linestyle='--', alpha=0.7, label='Max = 1')
ax2.legend()

plt.tight_layout()
plt.show()

print(f"\n‚öôÔ∏è PAR√ÅMETROS DE MINMAXSCALER:")
minmax_params = """
MinMaxScaler(
    feature_range=(0, 1),    # Rango objetivo [min, max]
    copy=True,              # ¬øCrear copia de los datos?
    clip=False              # ¬øRecortar valores fuera del rango?
)
"""
print(minmax_params)

print(f"üéØ PAR√ÅMETRO 'feature_range':")
# Demostrar diferentes rangos
rangos_ejemplos = [(0, 1), (-1, 1), (0, 100)]

for rango in rangos_ejemplos:
    scaler_custom = MinMaxScaler(feature_range=rango)
    edad_scaled_custom = scaler_custom.fit_transform(
        datos_diferentes_escalas['Edad'].reshape(-1, 1)
    )

    print(f"feature_range={rango}: Rango resultante=[{edad_scaled_custom.min():.2f}, "
          f"{edad_scaled_custom.max():.2f}]")

print(f"\nüéØ PAR√ÅMETRO 'clip':")
print("‚Ä¢ clip=False (default): Permite valores fuera del rango en datos nuevos")
print("‚Ä¢ clip=True: Fuerza valores dentro del rango [0,1]")

# Demostraci√≥n con valores extremos
scaler_no_clip = MinMaxScaler(clip=False)
scaler_clip = MinMaxScaler(clip=True)

# Datos de entrenamiento normales
datos_entrenamiento = np.array([10, 20, 30, 40, 50]).reshape(-1, 1)
scaler_no_clip.fit(datos_entrenamiento)
scaler_clip.fit(datos_entrenamiento)

# Valor extremo nuevo (fuera del rango original)
valor_extremo = np.array([100]).reshape(-1, 1)  # Mucho mayor que el m√°ximo (50)

resultado_no_clip = scaler_no_clip.transform(valor_extremo)
resultado_clip = scaler_clip.transform(valor_extremo)

print(f"\nDatos de entrenamiento: [10, 20, 30, 40, 50]")
print(f"Valor nuevo extremo: 100")
print(f"clip=False: {resultado_no_clip[0][0]:.2f} (puede ser > 1)")
print(f"clip=True:  {resultado_clip[0][0]:.2f} (forzado a <= 1)")

print(f"\nü•ä MINMAXSCALER vs STANDARDSCALER:")
comparison_table = """
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ASPECTO             ‚îÇ MinMaxScaler        ‚îÇ StandardScaler      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Rango resultado     ‚îÇ [0, 1] (o custom)   ‚îÇ Media=0, Std=1      ‚îÇ
‚îÇ Sensible a outliers ‚îÇ MUY sensible        ‚îÇ Menos sensible      ‚îÇ
‚îÇ Preserva forma      ‚îÇ S√≠                  ‚îÇ S√≠                  ‚îÇ
‚îÇ Interpretaci√≥n      ‚îÇ % del rango total   ‚îÇ Desviaciones de media‚îÇ
‚îÇ Usa con             ‚îÇ Redes neuronales    ‚îÇ SVM, Regr. Log√≠stica‚îÇ
‚îÇ                     ‚îÇ Datos acotados      ‚îÇ Datos con outliers  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""
print(comparison_table)

# Demostraci√≥n pr√°ctica de sensibilidad a outliers
print(f"\nüß™ EXPERIMENTO: SENSIBILIDAD A OUTLIERS")

# Datos normales con un outlier
datos_con_outlier = np.array([10, 12, 11, 13, 12, 14, 100])  # 100 es outlier
datos_sin_outlier = np.array([10, 12, 11, 13, 12, 14])

# MinMaxScaler
mm_con_outlier = MinMaxScaler().fit_transform(datos_con_outlier.reshape(-1, 1))
mm_sin_outlier = MinMaxScaler().fit_transform(datos_sin_outlier.reshape(-1, 1))

# StandardScaler
from sklearn.preprocessing import StandardScaler
std_con_outlier = StandardScaler().fit_transform(datos_con_outlier.reshape(-1, 1))
std_sin_outlier = StandardScaler().fit_transform(datos_sin_outlier.reshape(-1, 1))

print("Datos normales (10-14) escalados:")
print(f"MinMax sin outlier: {mm_sin_outlier[:-1].ravel()}")
print(f"MinMax con outlier: {mm_con_outlier[:-1].ravel()}")
print("üëÜ ¬°MinMax comprime mucho los datos normales por culpa del outlier!")

print(f"\nStandard sin outlier: {std_sin_outlier.ravel()}")
print(f"Standard con outlier: {std_con_outlier[:-1].ravel()}")
print("üëÜ StandardScaler es menos afectado por el outlier")

print(f"\nüí° CU√ÅNDO USAR CADA UNO:")
print("üéØ Usar MinMaxScaler cuando:")
print("‚Ä¢ Necesitas un rango espec√≠fico [0,1] o [-1,1]")
print("‚Ä¢ Datos no tienen outliers extremos")
print("‚Ä¢ Trabajas con redes neuronales o algoritmos que requieren [0,1]")
print("‚Ä¢ La interpretaci√≥n como 'porcentaje del rango' es √∫til")

print(f"\nüéØ Usar StandardScaler cuando:")
print("‚Ä¢ Tus datos tienen outliers")
print("‚Ä¢ Usas SVM, regresi√≥n log√≠stica, PCA")
print("‚Ä¢ La distribuci√≥n es aproximadamente normal")
print("‚Ä¢ No te importa el rango espec√≠fico de salida")

# Caso de uso real
print(f"\nüè† CASO DE USO REAL: PRECIOS DE CASAS")
precios_casas = np.array([150000, 200000, 180000, 220000, 5000000])  # Una casa muy cara
print(f"Precios originales: {precios_casas}")

# Con MinMaxScaler
mm_precios = MinMaxScaler().fit_transform(precios_casas.reshape(-1, 1))
print(f"MinMaxScaler: {mm_precios.ravel()}")
print("üëÜ Las casas normales quedan muy comprimidas (0.00-0.02)")

# Con StandardScaler
std_precios = StandardScaler().fit_transform(precios_casas.reshape(-1, 1))
print(f"StandardScaler: {std_precios.ravel()}")
print("üëÜ Mejor separaci√≥n entre casas normales (-0.4 a 0.2)")

print(f"\n‚úÖ MEJORES PR√ÅCTICAS:")
print("1. Analiza outliers antes de elegir scaler")
print("2. Para datos con outliers: StandardScaler")
print("3. Para datos sin outliers y rango espec√≠fico: MinMaxScaler")
print("4. Siempre fit() con datos de entrenamiento solamente")
print("5. Guarda el scaler para aplicar a datos nuevos")
```

---

## üîÑ Par√°metros de Validaci√≥n Cruzada Avanzada

### TimeSeriesSplit - Para Datos Temporales

**ANALOG√çA**: Imagina que eres un analista financiero prediciendo precios de acciones. No puedes usar datos del futuro para predecir el pasado - ¬°ser√≠a hacer trampa!

```python
from sklearn.model_selection import TimeSeriesSplit
import numpy as np

# EJEMPLO: Datos de ventas mensuales
dates = pd.date_range('2020-01-01', periods=24, freq='M')
sales = np.random.rand(24) * 1000 + np.arange(24) * 50  # Tendencia creciente

tscv = TimeSeriesSplit(n_splits=5)

print("üîç C√ìMO FUNCIONA TimeSeriesSplit:")
print("="*50)

for i, (train_idx, test_idx) in enumerate(tscv.split(sales)):
    print(f"üìä Fold {i+1}:")
    print(f"   Entrenamiento: meses {train_idx[0]+1} a {train_idx[-1]+1}")
    print(f"   Prueba: meses {test_idx[0]+1} a {test_idx[-1]+1}")
    print(f"   Ratio: {len(train_idx)} train / {len(test_idx)} test")
    print()

"""
SALIDA ESPERADA:
üìä Fold 1:
   Entrenamiento: meses 1 a 4
   Prueba: meses 5 a 8

üìä Fold 2:
   Entrenamiento: meses 1 a 8
   Prueba: meses 9 a 12

... y as√≠ sucesivamente
"""
```

**Par√°metros Clave:**

```python
TimeSeriesSplit(
    n_splits=5,           # ¬øCu√°ntas divisiones temporales?
    max_train_size=None,  # ¬øLimitar tama√±o de entrenamiento?
    test_size=None,       # ¬øTama√±o fijo de prueba?
    gap=0                 # ¬øEspacio entre train y test?
)

# EJEMPLO PR√ÅCTICO: Predicci√≥n de demanda
def analizar_demanda_temporal():
    # Simulamos datos de demanda con estacionalidad
    np.random.seed(42)
    n_meses = 36

    # Tendencia + estacionalidad + ruido
    tendencia = np.linspace(100, 200, n_meses)
    estacionalidad = 20 * np.sin(2 * np.pi * np.arange(n_meses) / 12)
    ruido = np.random.normal(0, 10, n_meses)

    demanda = tendencia + estacionalidad + ruido

    # Validaci√≥n cruzada temporal
    tscv = TimeSeriesSplit(n_splits=6, gap=1)  # gap=1 mes de separaci√≥n

    scores = []
    for train_idx, test_idx in tscv.split(demanda):
        # Modelo simple: promedio m√≥vil
        train_data = demanda[train_idx]
        test_data = demanda[test_idx]

        # Predicci√≥n: promedio de √∫ltimos 3 meses
        prediction = np.mean(train_data[-3:])
        actual = np.mean(test_data)

        error = abs(prediction - actual)
        scores.append(error)

    print(f"üéØ Error promedio: {np.mean(scores):.2f}")
    print(f"üìä Desviaci√≥n est√°ndar: {np.std(scores):.2f}")

    return scores

errores = analizar_demanda_temporal()
```

### LeaveOneOut - Validaci√≥n Exhaustiva

**ANALOG√çA**: Es como probar un examen donde cada pregunta es tu conjunto de prueba, y todas las dem√°s preguntas son tu material de estudio.

```python
from sklearn.model_selection import LeaveOneOut
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier

# EJEMPLO: Dataset peque√±o y valioso
X, y = make_classification(n_samples=20, n_features=5, random_state=42)

loo = LeaveOneOut()
rf = RandomForestClassifier(n_estimators=10, random_state=42)

print("üîç LEAVE-ONE-OUT EN ACCI√ìN:")
print("="*40)

scores = []
for i, (train_idx, test_idx) in enumerate(loo.split(X)):
    print(f"Iteraci√≥n {i+1}: Entrenando con {len(train_idx)} muestras, probando con 1")

    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    rf.fit(X_train, y_train)
    score = rf.score(X_test, y_test)
    scores.append(score)

print(f"\nüéØ Precisi√≥n promedio: {np.mean(scores):.3f}")
print(f"üìä Desviaci√≥n est√°ndar: {np.std(scores):.3f}")
print(f"üìà Precisiones individuales: {len(scores)} evaluaciones")

# CU√ÅNDO USAR LeaveOneOut:
print("\nüí° USA LeaveOneOut CUANDO:")
print("‚úÖ Tienes pocos datos (< 100 muestras)")
print("‚úÖ Cada muestra es muy valiosa")
print("‚úÖ Quieres la estimaci√≥n m√°s precisa posible")
print("‚ùå NO uses con datasets grandes (muy lento)")
```

## üéõÔ∏è Par√°metros de Ensemble Methods Avanzados

### VotingClassifier - Democracia en ML

**ANALOG√çA**: Es como un panel de expertos tomando una decisi√≥n. Un m√©dico, un ingeniero y un abogado dan sus opiniones, y la decisi√≥n final se basa en la mayor√≠a o en el promedio ponderado.

```python
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.datasets import make_classification

# CREAMOS UN CONJUNTO DE EXPERTOS
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Nuestros "expertos"
expert1 = LogisticRegression(random_state=42)           # El matem√°tico
expert2 = DecisionTreeClassifier(random_state=42)      # El l√≥gico
expert3 = SVC(probability=True, random_state=42)       # El te√≥rico

# VOTING HARD: Decisi√≥n por mayor√≠a simple
voting_hard = VotingClassifier(
    estimators=[
        ('logistic', expert1),
        ('tree', expert2),
        ('svm', expert3)
    ],
    voting='hard'  # Voto directo: 0 o 1
)

# VOTING SOFT: Decisi√≥n por promedio de probabilidades
voting_soft = VotingClassifier(
    estimators=[
        ('logistic', expert1),
        ('tree', expert2),
        ('svm', expert3)
    ],
    voting='soft',  # Promedio de probabilidades
    weights=[2, 1, 1]  # El matem√°tico cuenta doble
)

# COMPARACI√ìN
from sklearn.model_selection import cross_val_score

scores_hard = cross_val_score(voting_hard, X, y, cv=5)
scores_soft = cross_val_score(voting_soft, X, y, cv=5)

print("üó≥Ô∏è RESULTADOS DE VOTACI√ìN:")
print("="*30)
print(f"Voting Hard: {scores_hard.mean():.3f} ¬± {scores_hard.std():.3f}")
print(f"Voting Soft: {scores_soft.mean():.3f} ¬± {scores_soft.std():.3f}")

# EJEMPLO DETALLADO: C√≥mo votan los modelos
voting_soft.fit(X[:800], y[:800])  # Entrenar con parte de los datos

# Veamos las predicciones individuales
sample = X[800:805]  # 5 muestras de prueba

print("\nüîç AN√ÅLISIS DETALLADO DE VOTACI√ìN:")
print("="*40)

for i, estimator in voting_soft.estimators_:
    pred = estimator.predict(sample)
    prob = estimator.predict_proba(sample)
    print(f"\n{i.upper()}:")
    print(f"  Predicciones: {pred}")
    print(f"  Probabilidades clase 1: {prob[:, 1]}")

final_pred = voting_soft.predict(sample)
final_prob = voting_soft.predict_proba(sample)

print(f"\nüéØ DECISI√ìN FINAL:")
print(f"  Predicciones: {final_pred}")
print(f"  Probabilidades clase 1: {final_prob[:, 1]}")
```

### BaggingClassifier - Diversidad por Muestreo

**ANALOG√çA**: Es como hacer encuestas pol√≠ticas. En lugar de preguntar a las mismas 1000 personas, haces 10 encuestas diferentes a 1000 personas distintas (con posible solapamiento) y promedias los resultados.

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

# CONFIGURACI√ìN DEL BAGGING
bagging = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(),
    n_estimators=100,           # 100 "encuestas" diferentes
    max_samples=0.8,           # Cada encuesta usa 80% de los datos
    max_features=0.8,          # Cada encuesta usa 80% de las caracter√≠sticas
    bootstrap=True,            # Con reemplazo (misma persona puede aparecer 2 veces)
    bootstrap_features=False,  # Sin reemplazo en caracter√≠sticas
    random_state=42,
    n_jobs=-1                  # Paralelizaci√≥n completa
)

# COMPARACI√ìN: √Årbol individual vs Bagging
tree_individual = DecisionTreeClassifier(random_state=42)

# Evaluaci√≥n
scores_individual = cross_val_score(tree_individual, X, y, cv=5)
scores_bagging = cross_val_score(bagging, X, y, cv=5)

print("üå≥ √ÅRBOL INDIVIDUAL vs BAGGING:")
print("="*35)
print(f"√Årbol solo:     {scores_individual.mean():.3f} ¬± {scores_individual.std():.3f}")
print(f"Bagging (100):  {scores_bagging.mean():.3f} ¬± {scores_bagging.std():.3f}")

# AN√ÅLISIS DE LA DIVERSIDAD
bagging.fit(X, y)

print(f"\nüìä AN√ÅLISIS DE DIVERSIDAD:")
print(f"N√∫mero de estimadores entrenados: {len(bagging.estimators_)}")

# Veamos qu√© tan diferentes son las predicciones
sample_predictions = []
for estimator in bagging.estimators_[:10]:  # Solo los primeros 10
    pred = estimator.predict(X[:100])  # En las primeras 100 muestras
    sample_predictions.append(pred)

# Calculamos la diversidad (cu√°nto difieren las predicciones)
predictions_array = np.array(sample_predictions)
diversity_per_sample = np.std(predictions_array, axis=0)  # Desviaci√≥n por muestra

print(f"Diversidad promedio: {diversity_per_sample.mean():.3f}")
print(f"üìà Diversidad alta = Mayor robustez")
```

## üß† Par√°metros de Deep Learning en Scikit-Learn

### MLPClassifier - Redes Neuronales Simplificadas

**ANALOG√çA**: Una red neuronal es como una empresa con m√∫ltiples departamentos. Cada departamento (capa) procesa informaci√≥n y la pasa al siguiente. Los par√°metros determinan cu√°ntos empleados hay en cada departamento y c√≥mo trabajan.

```python
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import make_circles
from sklearn.preprocessing import StandardScaler

# PROBLEMA COMPLEJO: C√≠rculos conc√©ntricos
X, y = make_circles(n_samples=1000, noise=0.1, factor=0.5, random_state=42)

# ESCALADO (MUY IMPORTANTE para redes neuronales)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# RED NEURONAL B√ÅSICA
mlp_simple = MLPClassifier(
    hidden_layer_sizes=(10,),      # 1 capa oculta con 10 neuronas
    activation='relu',             # Funci√≥n de activaci√≥n
    solver='adam',                 # Algoritmo de optimizaci√≥n
    alpha=0.0001,                 # Regularizaci√≥n L2
    learning_rate_init=0.001,     # Tasa de aprendizaje inicial
    max_iter=500,                 # M√°ximo n√∫mero de √©pocas
    random_state=42
)

# RED NEURONAL COMPLEJA
mlp_complex = MLPClassifier(
    hidden_layer_sizes=(100, 50, 25),  # 3 capas: 100‚Üí50‚Üí25 neuronas
    activation='tanh',                  # Activaci√≥n diferente
    solver='lbfgs',                    # Mejor para datasets peque√±os
    alpha=0.01,                        # M√°s regularizaci√≥n
    learning_rate='adaptive',          # Tasa de aprendizaje adaptativa
    max_iter=1000,
    random_state=42
)

# EVALUACI√ìN Y COMPARACI√ìN
scores_simple = cross_val_score(mlp_simple, X_scaled, y, cv=5)
scores_complex = cross_val_score(mlp_complex, X_scaled, y, cv=5)

print("üß† COMPARACI√ìN DE REDES NEURONALES:")
print("="*40)
print(f"Red Simple (10):          {scores_simple.mean():.3f} ¬± {scores_simple.std():.3f}")
print(f"Red Compleja (100-50-25): {scores_complex.mean():.3f} ¬± {scores_complex.std():.3f}")

# AN√ÅLISIS DETALLADO DE PAR√ÅMETROS
print("\nüîç EXPLICACI√ìN DE PAR√ÅMETROS:")
print("="*35)

print("\nüèóÔ∏è ARQUITECTURA:")
print("hidden_layer_sizes=(100, 50, 25)")
print("  ‚Ü≥ Entrada ‚Üí 100 neuronas ‚Üí 50 neuronas ‚Üí 25 neuronas ‚Üí Salida")
print("  ‚Ü≥ Como una pir√°mide: informaci√≥n se condensa gradualmente")

print("\n‚ö° FUNCI√ìN DE ACTIVACI√ìN:")
activations = ['relu', 'tanh', 'logistic']
for act in activations:
    mlp_test = MLPClassifier(hidden_layer_sizes=(50,), activation=act,
                            max_iter=300, random_state=42)
    score = cross_val_score(mlp_test, X_scaled, y, cv=3).mean()
    print(f"  {act:8}: {score:.3f}")

print("\nüéì ALGORITMOS DE OPTIMIZACI√ìN:")
solvers = ['lbfgs', 'sgd', 'adam']
for solver in solvers:
    try:
        mlp_test = MLPClassifier(hidden_layer_sizes=(50,), solver=solver,
                                max_iter=300, random_state=42)
        score = cross_val_score(mlp_test, X_scaled, y, cv=3).mean()
        print(f"  {solver:6}: {score:.3f}")
    except:
        print(f"  {solver:6}: No compatible con este dataset")
```

### Par√°metros de Regularizaci√≥n en Detalle

```python
# EXPERIMENTO: Efecto de la regularizaci√≥n (alpha)
alphas = [0.0001, 0.001, 0.01, 0.1, 1.0]

print("\nüéõÔ∏è EFECTO DE LA REGULARIZACI√ìN (ALPHA):")
print("="*45)
print("Alpha\t| Train Score | Val Score | Diferencia")
print("-" * 45)

for alpha in alphas:
    mlp = MLPClassifier(
        hidden_layer_sizes=(100, 50),
        alpha=alpha,
        max_iter=500,
        random_state=42
    )

    # Entrenamos y evaluamos
    mlp.fit(X_scaled[:700], y[:700])  # 70% para entrenamiento

    train_score = mlp.score(X_scaled[:700], y[:700])
    val_score = mlp.score(X_scaled[700:], y[700:])
    difference = train_score - val_score

    print(f"{alpha:5.4f}\t| {train_score:.3f}     | {val_score:.3f}     | {difference:.3f}")

print("\nüí° INTERPRETACI√ìN:")
print("‚úÖ Diferencia peque√±a = Buen balance")
print("‚ùå Train >> Val = Overfitting (alpha muy bajo)")
print("‚ùå Ambos bajos = Underfitting (alpha muy alto)")
```

## üéØ Conclusi√≥n Expandida

Esta gu√≠a exhaustiva te ha mostrado los par√°metros m√°s importantes en Machine Learning con analog√≠as del mundo real, ejemplos pr√°cticos detallados y an√°lisis profundos. Ahora entiendes que cada par√°metro es como una palanca de control que afecta el comportamiento de tu modelo.

### üöÄ Pasos para Dominar los Par√°metros

1. **EXPERIMENTA**: Cambia un par√°metro a la vez y observa el efecto
2. **DOCUMENTA**: Lleva un registro de qu√© combinaciones funcionan mejor
3. **VALIDA**: Siempre usa validaci√≥n cruzada para evaluar cambios
4. **COMPARA**: Benchmarkea diferentes configuraciones sistem√°ticamente
5. **INTERPRETA**: Entiende por qu√© ciertos par√°metros mejoran el rendimiento

### üé™ Recuerda las Analog√≠as Clave

- **n_estimators**: M√°s consultores = mejor decisi√≥n (hasta un punto)
- **max_depth**: Profundidad de preguntas en 20 preguntas
- **learning_rate**: Velocidad de aprendizaje como velocidad de conducci√≥n
- **alpha**: Regularizaci√≥n como restricciones de velocidad
- **Cross-validation**: Ex√°menes de pr√°ctica antes del examen final
- **GridSearch**: B√∫squeda exhaustiva como probar todas las pizzas
- **VotingClassifier**: Panel de expertos tomando decisiones
- **Bagging**: M√∫ltiples encuestas para mayor precisi√≥n
- **TimeSeriesSplit**: No usar el futuro para predecir el pasado

### üèÜ Principios Universales

1. **No hay par√°metros perfectos universales** - cada dataset es √∫nico
2. **M√°s complejo ‚â† mejor** - a veces la simplicidad gana
3. **La validaci√≥n cruzada es sagrada** - nunca conf√≠es en una sola evaluaci√≥n
4. **El overfitting es el enemigo silencioso** - siempre mantente alerta
5. **La interpretabilidad tiene valor** - a veces necesitas explicar tus decisiones
6. **El escalado de datos es crucial** - especialmente para redes neuronales y SVM
7. **Los outliers afectan mucho** - anal√≠zalos antes de elegir scalers

### üìö Recursos Adicionales para Seguir Aprendiendo

- **Practica con datasets reales**: Kaggle, UCI ML Repository
- **Experimenta con diferentes combinaciones**: No te quedes con los defaults
- **Lee la documentaci√≥n**: Scikit-learn tiene excelente documentaci√≥n
- **Participa en competencias**: Kaggle, DrivenData
- **Construye tu propio pipeline**: Desde datos crudos hasta modelo en producci√≥n

¬°Ahora tienes las herramientas y el conocimiento para convertirte en un maestro de los par√°metros de Machine Learning! üéì‚ú®

**Recuerda**: La teor√≠a sin pr√°ctica es est√©ril, pero la pr√°ctica sin teor√≠a es ciega. ¬°Combina ambas y ser√°s imparable! üöÄ